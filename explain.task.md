# ServerParameterMetadata English Explanation Update Plan

This document tracks the systematic update of English explanations in ServerParameterMetadata.vb to match the comprehensive detail level found in README.server.md.

## Status Legend
- [ ] Pending
- [x] Completed
- [ğŸ”„] In Progress

---

## Chunk 1: Common Parameters (Part 1) - Help & Basic Configuration
**Status: [ ] Pending**

1. **`-h, --help, --usage`**
   - Current: "Print usage and exit | æ‰“å°ä½¿ç”¨è¯´æ˜å¹¶é€€å‡º"
   - README: "print usage and exit"
   - Target: "Print usage and exit | æ‰“å°ä½¿ç”¨è¯´æ˜å¹¶é€€å‡ºã€‚æ˜¾ç¤ºå®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°åˆ—è¡¨å’Œç®€çŸ­æè¿°ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£æ‰€æœ‰å¯ç”¨çš„é…ç½®é€‰é¡¹ã€‚åŒ…å«æ‰€æœ‰å‚æ•°çš„åŸºæœ¬ç”¨æ³•ã€é»˜è®¤å€¼å’Œç®€çŸ­è¯´æ˜ï¼Œæ˜¯è·å–å¸®åŠ©ä¿¡æ¯çš„ä¸»è¦æ–¹å¼ã€‚"

2. **`--version`**
   - Current: "Show version and build info | æ˜¾ç¤ºç‰ˆæœ¬å’Œæ„å»ºä¿¡æ¯"
   - README: "show version and build info"
   - Target: "Show version and build info | æ˜¾ç¤ºç‰ˆæœ¬å’Œæ„å»ºä¿¡æ¯ã€‚è¾“å‡ºå½“å‰è½¯ä»¶çš„ç‰ˆæœ¬å·ã€æ„å»ºæ—¥æœŸã€æäº¤ä¿¡æ¯ç­‰è¯¦ç»†ä¿¡æ¯ã€‚ç”¨äºç‰ˆæœ¬ç¡®è®¤ã€é—®é¢˜è¯Šæ–­å’ŒåŠŸèƒ½å…¼å®¹æ€§æ£€æŸ¥ï¼Œç¡®ä¿ç”¨æˆ·äº†è§£æ­£åœ¨ä½¿ç”¨çš„å…·ä½“ç‰ˆæœ¬ã€‚"

3. **`--completion-bash`**
   - Current: "Print source-able bash completion script for llama.cpp | æ‰“å°å¯ç”¨äº source çš„ llama.cpp bash å®Œæˆè„šæœ¬"
   - README: "print source-able bash completion script for llama.cpp"
   - Target: "Print source-able bash completion script for llama.cpp | æ‰“å°å¯ç”¨äº source çš„ llama.cpp bash å®Œæˆè„šæœ¬ã€‚è¾“å‡º bash shell çš„å‘½ä»¤è¡Œè‡ªåŠ¨å®Œæˆè„šæœ¬ï¼Œå¯ç”¨äºé…ç½® shell è‡ªåŠ¨å®ŒæˆåŠŸèƒ½ã€‚æä¾›æ›´ä¾¿æ·çš„å‘½ä»¤è¡Œä½¿ç”¨ä½“éªŒï¼Œå‡å°‘æ‰‹åŠ¨è¾“å…¥å‚æ•°çš„å·¥ä½œé‡ã€‚"

4. **`--verbose-prompt`**
   - Current: "Verbose prompt | è¯¦ç»†æç¤º"
   - README: "print a verbose prompt before generation (default: false)"
   - Target: "Print a verbose prompt before generation (default: false) | åœ¨ç”Ÿæˆä¹‹å‰æ‰“å°è¯¦ç»†çš„æç¤ºä¿¡æ¯ï¼Œæ˜¾ç¤ºå®é™…å‘é€ç»™æ¨¡å‹çš„å®Œæ•´æç¤ºå†…å®¹ã€‚ç”¨äºè°ƒè¯•å’ŒéªŒè¯æç¤ºæ„å»ºçš„æ­£ç¡®æ€§ã€‚"

5. **`-t, --threads N`**
   - Current: "Number of threads to use | ä½¿ç”¨çš„çº¿ç¨‹æ•°é‡"
   - README: "number of threads to use during generation (default: -1)<br/>(env: LLAMA_ARG_THREADS)"
   - Target: "Number of threads to use during generation (default: -1) (env: LLAMA_ARG_THREADS) | ç”Ÿæˆè¿‡ç¨‹ä¸­ä½¿ç”¨çš„çº¿ç¨‹æ•°ã€‚æŒ‡å®šæ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„ CPU çº¿ç¨‹æ•°ï¼Œç›´æ¥å½±å“æ¨ç†é€Ÿåº¦å’Œ CPU ä½¿ç”¨ç‡ã€‚å»ºè®®è®¾ç½®ä¸º CPU é€»è¾‘æ ¸å¿ƒæ•°ã€‚é»˜è®¤ä¸º -1 è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ã€‚"

6. **`-tb, --threads-batch N`**
   - Current: "Threads for batch processing | æ‰¹å¤„ç†çº¿ç¨‹æ•°"
   - README: "number of threads to use during batch and prompt processing (default: same as --threads)"
   - Target: "Number of threads to use during batch and prompt processing (default: same as --threads) | æ‰¹å¤„ç†å’Œæç¤ºè¯å¤„ç†æœŸé—´ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼Œä¸“é—¨ç”¨äºæ‰¹å¤„ç†å’Œæç¤ºè¯å¤„ç†çš„çº¿ç¨‹æ•°ï¼Œé€šå¸¸ä¸ç”Ÿæˆçº¿ç¨‹åˆ†å¼€è®¾ç½®ã€‚é»˜è®¤ä¸ --threads ç›¸åŒã€‚"

7. **`-C, --cpu-mask M`**
   - Current: "CPU mask | CPU æ©ç "
   - README: "CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: "")"
   - Target: "CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: "") | CPU äº²å’Œæ€§çš„åå…­è¿›åˆ¶æ©ç ï¼Œä¸ cpu-range äº’è¡¥ã€‚ä»»æ„é•¿åº¦çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼Œç”¨äºç²¾ç¡®æ§åˆ¶è¿›ç¨‹åœ¨ç‰¹å®š CPU æ ¸å¿ƒä¸Šè¿è¡Œã€‚"

8. **`-Cr, --cpu-range lo-hi`**
   - Current: "CPU range | CPU èŒƒå›´"
   - README: "range of CPUs for affinity. Complements --cpu-mask"
   - Target: "Range of CPUs for affinity. Complements --cpu-mask | CPU äº²å’Œæ€§çš„èŒƒå›´ï¼Œæ ¼å¼ä¸º lo-hiã€‚ä¸ --cpu-mask äº’è¡¥ï¼Œç”¨äºæŒ‡å®šè¿›ç¨‹å¯ä½¿ç”¨çš„ CPU æ ¸å¿ƒèŒƒå›´ã€‚"

9. **`--cpu-strict <0|1>`**
   - Current: "Strict CPU affinity | ä¸¥æ ¼ CPU äº²å’Œæ€§"
   - README: "use strict CPU placement (default: 0)<br/>"
   - Target: "Use strict CPU placement (default: 0) | ä½¿ç”¨ä¸¥æ ¼çš„ CPU æ”¾ç½®ï¼Œç¡®ä¿è¿›ç¨‹åªåœ¨æŒ‡å®šçš„ CPU ä¸Šè¿è¡Œã€‚æé«˜æ€§èƒ½ç¨³å®šæ€§ï¼Œä½†å¯èƒ½é™ä½çµæ´»æ€§ã€‚"

10. **`--prio N`**
    - Current: "Process priority | è¿›ç¨‹ä¼˜å…ˆçº§"
    - README: "set process/thread priority : low(-1), normal(0), medium(1), high(2), realtime(3) (default: 0)<br/>"
    - Target: "Set process/thread priority : low(-1), normal(0), medium(1), high(2), realtime(3) (default: 0) | è®¾ç½®è¿›ç¨‹/çº¿ç¨‹ä¼˜å…ˆçº§ï¼šlow(-1)ã€normal(0)ã€medium(1)ã€high(2)ã€realtime(3)ã€‚è¾ƒé«˜çš„ä¼˜å…ˆçº§å¯è·å¾—æ›´å¤š CPU æ—¶é—´ï¼Œä½†å¯èƒ½å½±å“ç³»ç»Ÿç¨³å®šæ€§ã€‚"

11. **`--poll <0...100>`**
    - Current: "Poll interval | è½®è¯¢é—´éš”"
    - README: "use polling level to wait for work (0 - no polling, default: 50)<br/>"
    - Target: "Use polling level to wait for work (0 - no polling, default: 50) | ä½¿ç”¨è½®è¯¢çº§åˆ«ç­‰å¾…å·¥ä½œï¼ˆ0 - æ— è½®è¯¢ï¼‰ã€‚å½±å“å“åº”é€Ÿåº¦å’Œ CPU ä½¿ç”¨ç‡çš„å¹³è¡¡ã€‚è¾ƒé«˜çš„è½®è¯¢çº§åˆ«å¯æé«˜å“åº”é€Ÿåº¦ä½†å¢åŠ  CPU ä½¿ç”¨ã€‚"

12. **`-Cb, --cpu-mask-batch M`**
    - Current: "CPU mask for batch | æ‰¹å¤„ç† CPU æ©ç "
    - README: "CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask)"
    - Target: "CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask) | æ‰¹å¤„ç†æ“ä½œçš„ CPU äº²å’Œæ€§æ©ç ï¼Œä¸ --cpu-mask-batch äº’è¡¥ã€‚é»˜è®¤ä¸ --cpu-mask ç›¸åŒï¼Œç”¨äºä¼˜åŒ–æ‰¹å¤„ç†æ€§èƒ½ã€‚"

13. **`-Crb, --cpu-range-batch lo-hi`**
    - Current: "CPU range for batch | æ‰¹å¤„ç† CPU èŒƒå›´"
    - README: "ranges of CPUs for affinity. Complements --cpu-mask-batch"
    - Target: "Ranges of CPUs for affinity. Complements --cpu-mask-batch | æ‰¹å¤„ç†æ“ä½œçš„ CPU äº²å’Œæ€§èŒƒå›´ï¼Œä¸ --cpu-mask-batch äº’è¡¥ã€‚ç”¨äºä¼˜åŒ–æ‰¹å¤„ç†æ“ä½œçš„ CPU åˆ†é…ã€‚"

14. **`--cpu-strict-batch <0|1>`**
    - Current: "Strict CPU affinity for batch | æ‰¹å¤„ç†ä¸¥æ ¼ CPU äº²å’Œæ€§"
    - README: "use strict CPU placement (default: same as --cpu-strict)"
    - Target: "Use strict CPU placement (default: same as --cpu-strict) | æ‰¹å¤„ç†æ“ä½œçš„ä¸¥æ ¼ CPU æ”¾ç½®ï¼Œé»˜è®¤ä¸ --cpu-strict ç›¸åŒã€‚ç¡®ä¿æ‰¹å¤„ç†ä»»åŠ¡åœ¨æŒ‡å®šçš„ CPU ä¸Šè¿è¡Œã€‚"

15. **`--prio-batch N`**
    - Current: "Process priority for batch | æ‰¹å¤„ç†è¿›ç¨‹ä¼˜å…ˆçº§"
    - README: "set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)<br/>"
    - Target: "Set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0) | æ‰¹å¤„ç†æ“ä½œçš„è¿›ç¨‹/çº¿ç¨‹ä¼˜å…ˆçº§ï¼š0-normalã€1-mediumã€2-highã€3-realtimeã€‚ç”¨äºä¼˜åŒ–æ‰¹å¤„ç†ä»»åŠ¡çš„è°ƒåº¦ã€‚"

16. **`--poll-batch <0|1>`**
    - Current: "Poll interval for batch | æ‰¹å¤„ç†è½®è¯¢é—´éš”"
    - README: "use polling to wait for work (default: same as --poll)"
    - Target: "Use polling to wait for work (default: same as --poll) | æ‰¹å¤„ç†æ“ä½œçš„è½®è¯¢çº§åˆ«ï¼Œé»˜è®¤ä¸ --poll ç›¸åŒã€‚ç”¨äºä¼˜åŒ–æ‰¹å¤„ç†ä»»åŠ¡çš„å“åº”é€Ÿåº¦å’Œ CPU ä½¿ç”¨ã€‚"

17. **`-c, --ctx-size N`**
    - Current: "Context size | ä¸Šä¸‹æ–‡çª—å£å¤§å°"
    - README: "size of the prompt context (default: 4096, 0 = loaded from model)<br/>(env: LLAMA_ARG_CTX_SIZE)"
    - Target: "Size of the prompt context (default: 4096, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE) | ä¸Šä¸‹æ–‡çª—å£å¤§å°ã€‚æŒ‡å®šæç¤ºè¯ä¸Šä¸‹æ–‡çš„æœ€å¤§ token æ•°é‡ï¼Œå½±å“æ¨¡å‹èƒ½å¤„ç†çš„è¾“å…¥é•¿åº¦ã€‚æ›´å¤§çš„ä¸Šä¸‹æ–‡éœ€è¦æ›´å¤šå†…å­˜ã€‚é»˜è®¤ä¸º 4096ï¼Œ0 è¡¨ç¤ºä»æ¨¡å‹åŠ è½½ã€‚"

18. **`-n, --predict, --n-predict N`**
    - Current: "Number of tokens to predict | é¢„æµ‹çš„ token æ•°é‡"
    - README: "number of tokens to predict (default: -1, -1 = infinity)<br/>(env: LLAMA_ARG_N_PREDICT)"
    - Target: "Number of tokens to predict (default: -1, -1 = infinity) (env: LLAMA_ARG_N_PREDICT) | é¢„æµ‹çš„ token æ•°é‡ã€‚æ§åˆ¶æ¯æ¬¡ç”Ÿæˆè¾“å‡ºçš„æœ€å¤§ token æ•°ï¼Œé˜²æ­¢æ— é™ç”Ÿæˆã€‚-1 è¡¨ç¤ºæ— é™åˆ¶ã€‚é€‚ç”¨äºèŠå¤©ã€ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ã€‚"

19. **`-b, --batch-size N`**
    - Current: "Batch size for processing | æ‰¹å¤„ç†å¤§å°"
    - README: "logical maximum batch size (default: 2048)<br/>(env: LLAMA_ARG_BATCH)"
    - Target: "Logical maximum batch size (default: 2048) (env: LLAMA_ARG_BATCH) | é€»è¾‘æœ€å¤§æ‰¹å¤„ç†å¤§å°ã€‚å½±å“åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°é‡ã€‚è¾ƒå¤§çš„æ‰¹å¤„ç†å¯æé«˜ååé‡ä½†å¢åŠ å†…å­˜ä½¿ç”¨ã€‚é€‚ç”¨äºå¤šç”¨æˆ·å¹¶å‘åœºæ™¯ã€‚"

20. **`-ub, --ubatch-size N`**
    - Current: "Micro batch size | å¾®æ‰¹å¤„ç†å¤§å°"
    - README: "physical maximum batch size (default: 512)<br/>(env: LLAMA_ARG_UBATCH)"
    - Target: "Physical maximum batch size (default: 512) (env: LLAMA_ARG_UBATCH) | ç‰©ç†æœ€å¤§æ‰¹å¤„ç†å¤§å°ã€‚å®é™…å¤„ç†æ—¶çš„æœ€å°å•ä½ã€‚è¾ƒå°çš„å¾®æ‰¹å¤„ç†å¯æé«˜å†…å­˜æ•ˆç‡ï¼Œä½†å¯èƒ½ç•¥å¾®é™ä½æ€§èƒ½ã€‚"

---

## Chunk 2: Common Parameters (Part 2) - Memory & Cache Management
**Status: [ ] Pending**

1. **`--keep N`**
   - Current: "Keep model in memory | ä¿ç•™åˆå§‹æç¤º token æ•°é‡"
   - README: "number of tokens to keep from the initial prompt (default: 0, -1 = all)"
   - Target: "Number of tokens to keep from the initial prompt (default: 0, -1 = all) | ä»åˆå§‹æç¤ºä¸­ä¿ç•™çš„ token æ•°é‡ï¼Œ0 è¡¨ç¤ºä¸ä¿ç•™ï¼Œ-1 è¡¨ç¤ºå…¨éƒ¨ä¿ç•™ã€‚ç”¨äºç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡å’ŒçŠ¶æ€ï¼Œç¡®ä¿è¿è´¯çš„äº¤äº’ä½“éªŒã€‚"

2. **`--swa-full`**
   - Current: "Sliding window attention full | å…¨å°ºå¯¸æ»‘åŠ¨çª—å£æ³¨æ„åŠ›"
   - README: "use full-size SWA cache (default: false)<br/>[(more info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)<br/>(env: LLAMA_ARG_SWA_FULL)"
   - Target: "Use full-size SWA cache (default: false) (env: LLAMA_ARG_SWA_FULL) | ä½¿ç”¨å®Œæ•´å¤§å°çš„ SWA ç¼“å­˜ï¼Œæé«˜é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„æ€§èƒ½å’Œå†…å­˜æ•ˆç‡ã€‚é€‚ç”¨äºå¤„ç†è¶…é•¿æ–‡æœ¬æˆ–éœ€è¦å¤§é‡å†å²ä¸Šä¸‹æ–‡çš„åœºæ™¯ã€‚"

3. **`--kv-unified, -kvu`**
   - Current: "Unified KV cache | ç»Ÿä¸€ KV ç¼“å­˜"
   - README: "use single unified KV buffer for the KV cache of all sequences (default: false)<br/>[(more info)](https://github.com/ggml-org/llama.cpp/pull/14363)<br/>(env: LLAMA_ARG_KV_SPLIT)"
   - Target: "Use single unified KV buffer for the KV cache of all sequences (default: false) (env: LLAMA_ARG_KV_SPLIT) | ä½¿ç”¨å•ä¸€ç»Ÿä¸€çš„ KV ç¼“å†²åŒºå­˜å‚¨æ‰€æœ‰åºåˆ—çš„ KV ç¼“å­˜ï¼Œè€Œä¸æ˜¯ä¸ºæ¯ä¸ªåºåˆ—åˆ†åˆ«åˆ†é…ã€‚å¯æé«˜å†…å­˜æ•ˆç‡ï¼Œä½†å¯èƒ½åœ¨æŸäº›åœºæ™¯ä¸‹å½±å“æ€§èƒ½ã€‚"

4. **`-fa, --flash-attn`**
   - Current: "Enable flash attention | å¯ç”¨ Flash Attention"
   - README: "enable Flash Attention (default: disabled)<br/>(env: LLAMA_ARG_FLASH_ATTN)"
   - Target: "Enable Flash Attention (default: disabled) (env: LLAMA_ARG_FLASH_ATTN) | å¯ç”¨ Flash Attention ä¼˜åŒ–ç®—æ³•ï¼Œæ˜¾è‘—æé«˜æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ—¶é—´ã€‚éœ€è¦ç¡¬ä»¶æ”¯æŒï¼Œå¯å¤§å¹…æå‡é•¿æ–‡æœ¬å¤„ç†æ€§èƒ½ã€‚"

5. **`--no-perf`**
   - Current: "Disable performance metrics | ç¦ç”¨æ€§èƒ½æŒ‡æ ‡"
   - README: "disable internal libllama performance timings (default: false)<br/>(env: LLAMA_ARG_NO_PERF)"
   - Target: "Disable internal libllama performance timings (default: false) (env: LLAMA_ARG_NO_PERF) | ç¦ç”¨å†…éƒ¨ libllama æ€§èƒ½è®¡æ—¶ï¼Œå‡å°‘å¼€é”€ä½†å¤±å»æ€§èƒ½æ•°æ®ã€‚åœ¨ä¸éœ€è¦æ€§èƒ½åˆ†æçš„ç”Ÿäº§ç¯å¢ƒä¸­å¯ç•¥å¾®æé«˜æ€§èƒ½ã€‚"

6. **`-e, --escape`**
   - Current: "Escape special characters | è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦"
   - README: "process escapes sequences (\n, \r, \t, \', \", \\) (default: true)"
   - Target: "Process escapes sequences (\n, \r, \t, \', \", \\) (default: true) | å¤„ç†è½¬ä¹‰åºåˆ—ã€‚ç¡®ä¿æ–‡æœ¬è¾“å‡ºçš„æ­£ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œé˜²æ­¢æ ¼å¼é—®é¢˜å’Œæ³¨å…¥æ”»å‡»ã€‚"

7. **`--no-escape`**
   - Current: "Do not process escape sequences"
   - README: "do not process escape sequences"
   - Target: "Do not process escape sequences | ä¸å¤„ç†è½¬ä¹‰åºåˆ—ã€‚åœ¨æŸäº›éœ€è¦åŸå§‹æ–‡æœ¬è¾“å‡ºçš„åœºæ™¯ä¸­ä½¿ç”¨ã€‚"

8. **`--rope-scaling {none,linear,yarn}`**
   - Current: "RoPE scaling type | RoPE ç¼©æ”¾ç±»å‹"
   - README: "RoPE frequency scaling method, defaults to linear unless specified by the model<br/>(env: LLAMA_ARG_ROPE_SCALING_TYPE)"
   - Target: "RoPE frequency scaling method, defaults to linear unless specified by the model (env: LLAMA_ARG_ROPE_SCALING_TYPE) | Rotary Position Embedding é¢‘ç‡ç¼©æ”¾æ–¹æ³•ï¼Œå¯é€‰å€¼ï¼šnoneã€linearã€yarnã€‚ç”¨äºæ‰©å±•æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæé«˜é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚"

9. **`--rope-scale N`**
   - Current: "RoPE scale factor | RoPE ç¼©æ”¾å› å­"
   - README: "RoPE context scaling factor, expands context by a factor of N<br/>(env: LLAMA_ARG_ROPE_SCALE)"
   - Target: "RoPE context scaling factor, expands context by a factor of N (env: LLAMA_ARG_ROPE_SCALE) | RoPE ä¸Šä¸‹æ–‡ç¼©æ”¾å› å­ï¼Œå°†ä¸Šä¸‹æ–‡æ‰©å±• N å€ã€‚ä¾‹å¦‚ 2.0 è¡¨ç¤ºå°†ä¸Šä¸‹æ–‡é•¿åº¦ç¿»å€ã€‚é€‚ç”¨äºå¤„ç†è¶…å‡ºåŸå§‹è®­ç»ƒé•¿åº¦çš„é•¿æ–‡æœ¬ã€‚"

10. **`--rope-freq-base N`**
    - Current: "RoPE frequency base | RoPE é¢‘ç‡åŸºç¡€"
    - README: "RoPE base frequency, used by NTK-aware scaling (default: loaded from model)<br/>(env: LLAMA_ARG_ROPE_FREQ_BASE)"
    - Target: "RoPE base frequency, used by NTK-aware scaling (default: loaded from model) (env: LLAMA_ARG_ROPE_FREQ_BASE) | RoPE åŸºç¡€é¢‘ç‡ï¼Œç”¨äº NTK æ„ŸçŸ¥ç¼©æ”¾ã€‚å½±å“ä½ç½®ç¼–ç çš„é¢‘ç‡åˆ†å¸ƒï¼Œå¯è°ƒæ•´ä»¥é€‚åº”ä¸åŒçš„ä¸Šä¸‹æ–‡æ‰©å±•éœ€æ±‚ã€‚"

11. **`--rope-freq-scale N`**
    - Current: "RoPE frequency scale | RoPE é¢‘ç‡ç¼©æ”¾"
    - README: "RoPE frequency scaling factor, expands context by a factor of 1/N<br/>(env: LLAMA_ARG_ROPE_FREQ_SCALE)"
    - Target: "RoPE frequency scaling factor, expands context by a factor of 1/N (env: LLAMA_ARG_ROPE_FREQ_SCALE) | RoPE é¢‘ç‡ç¼©æ”¾å› å­ï¼Œå°†ä¸Šä¸‹æ–‡æ‰©å±• 1/N å€ã€‚ä¸ rope-scale ç›¸åä½œç”¨ï¼Œç”¨äºç²¾ç»†è°ƒæ•´ä½ç½®ç¼–ç çš„ç¼©æ”¾æ¯”ä¾‹ã€‚"

12. **`--yarn-orig-ctx N`**
    - Current: "YARN original context | YARN åŸå§‹ä¸Šä¸‹æ–‡"
    - README: "YaRN: original context size of model (default: 0 = model training context size)<br/>(env: LLAMA_ARG_YARN_ORIG_CTX)"
    - Target: "YaRN: original context size of model (default: 0 = model training context size) (env: LLAMA_ARG_YARN_ORIG_CTX) | YaRNï¼šæ¨¡å‹çš„åŸå§‹ä¸Šä¸‹æ–‡å¤§å°ï¼Œ0 è¡¨ç¤ºæ¨¡å‹è®­ç»ƒä¸Šä¸‹æ–‡å¤§å°ã€‚ç”¨äº YaRN æ‰©å±•æŠ€æœ¯ï¼Œç¡®ä¿ä½ç½®ç¼–ç çš„æ­£ç¡®ç¼©æ”¾ã€‚"

13. **`--yarn-ext-factor N`**
    - Current: "YARN extension factor | YARN æ‰©å±•å› å­"
    - README: "YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation)<br/>(env: LLAMA_ARG_YARN_EXT_FACTOR)"
    - Target: "YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation) (env: LLAMA_ARG_YARN_EXT_FACTOR) | YaRNï¼šå¤–æ¨æ··åˆå› å­ï¼Œ-1.0 è¡¨ç¤ºé»˜è®¤ï¼Œ0.0 è¡¨ç¤ºå®Œå…¨æ’å€¼ã€‚æ§åˆ¶ YaRN å¦‚ä½•å¤„ç†è¶…å‡ºåŸå§‹ä¸Šä¸‹æ–‡èŒƒå›´çš„å†…å®¹ï¼Œå½±å“é•¿æ–‡æœ¬å¤„ç†è´¨é‡ã€‚"

14. **`--yarn-attn-factor N`**
    - Current: "YARN attention factor | YARN æ³¨æ„åŠ›å› å­"
    - README: "YaRN: scale sqrt(t) or attention magnitude (default: 1.0)<br/>(env: LLAMA_ARG_YARN_ATTN_FACTOR)"
    - Target: "YaRN: scale sqrt(t) or attention magnitude (default: 1.0) (env: LLAMA_ARG_YARN_ATTN_FACTOR) | YaRNï¼šç¼©æ”¾ sqrt(t) æˆ–æ³¨æ„åŠ›å¹…åº¦ã€‚å½±å“æ³¨æ„åŠ›æœºåˆ¶åœ¨é•¿è·ç¦»ä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°ï¼Œæ§åˆ¶æ³¨æ„åŠ›å¾—åˆ†çš„ç¼©æ”¾ã€‚"

15. **`--yarn-beta-slow N`**
    - Current: "YARN beta slow | YARN æ…¢é€Ÿ beta"
    - README: "YaRN: high correction dim or alpha (default: 1.0)<br/>(env: LLAMA_ARG_YARN_BETA_SLOW)"
    - Target: "YaRN: high correction dim or alpha (default: 1.0) (env: LLAMA_ARG_YARN_BETA_SLOW) | YaRNï¼šé«˜æ ¡æ­£ç»´åº¦æˆ– alphaã€‚æ§åˆ¶ YaRN ä¸­è¾ƒæ…¢å˜åŒ–çš„æ ¡æ­£å‚æ•°ï¼Œå½±å“ä½ç½®ç¼–ç çš„é•¿æœŸè¡Œä¸ºã€‚"

16. **`--yarn-beta-fast N`**
    - Current: "YARN beta fast | YARN å¿«é€Ÿ beta"
    - README: "YaRN: low correction dim or beta (default: 32.0)<br/>(env: LLAMA_ARG_YARN_BETA_FAST)"
    - Target: "YaRN: low correction dim or beta (default: 32.0) (env: LLAMA_ARG_YARN_BETA_FAST) | YaRNï¼šä½æ ¡æ­£ç»´åº¦æˆ– betaã€‚æ§åˆ¶ YaRN ä¸­å¿«é€Ÿå˜åŒ–çš„æ ¡æ­£å‚æ•°ï¼Œå½±å“ä½ç½®ç¼–ç çš„çŸ­æœŸç²¾åº¦ã€‚"

17. **`-nkvo, --no-kv-offload`**
    - Current: "Disable KV cache offload | ç¦ç”¨ KV ç¼“å­˜å¸è½½"
    - README: "disable KV offload<br/>(env: LLAMA_ARG_NO_KV_OFFLOAD)"
    - Target: "Disable KV offload (env: LLAMA_ARG_NO_KV_OFFLOAD) | ç¦ç”¨ KV ç¼“å­˜å¸è½½ã€‚é˜»æ­¢å°† KV ç¼“å­˜å¸è½½åˆ° GPUï¼Œå¼ºåˆ¶æ‰€æœ‰ KV ç¼“å­˜ä¿ç•™åœ¨ CPU å†…å­˜ä¸­ã€‚åœ¨ GPU å†…å­˜æœ‰é™æ—¶å¯ç”¨ï¼Œä½†å¯èƒ½å½±å“æ¨ç†é€Ÿåº¦ã€‚"

18. **`-nr, --no-repack`**
    - Current: "Disable repacking | ç¦ç”¨æƒé‡é‡æ–°æ‰“åŒ…"
    - README: "disable weight repacking<br/>(env: LLAMA_ARG_NO_REPACK)"
    - Target: "Disable weight repacking (env: LLAMA_ARG_NO_REPACK) | ç¦ç”¨æƒé‡é‡æ–°æ‰“åŒ…ã€‚ä¸é‡æ–°æ‰“åŒ…æƒé‡ï¼Œå¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹åŠ å¿«åŠ è½½é€Ÿåº¦ï¼Œä½†å¯èƒ½å½±å“åç»­æ¨ç†æ€§èƒ½ã€‚é€šå¸¸å»ºè®®å¯ç”¨é‡æ–°æ‰“åŒ…ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚"

19. **`-ctk, --cache-type-k TYPE`**
    - Current: "Cache type for K | K ç¼“å­˜ç±»å‹"
    - README: "KV cache data type for K<br/>allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1<br/>(default: f16)<br/>(env: LLAMA_ARG_CACHE_TYPE_K)"
    - Target: "KV cache data type for K (env: LLAMA_ARG_CACHE_TYPE_K) | KV ç¼“å­˜ä¸­ K é”®çš„æ•°æ®ç±»å‹ï¼Œå¯é€‰å€¼ï¼šf32ã€f16ã€bf16ã€q8_0ã€q4_0ã€q4_1ã€iq4_nlã€q5_0ã€q5_1ã€‚å½±å“å†…å­˜å ç”¨å’Œç²¾åº¦ï¼Œf16 æ˜¯è¾ƒå¥½çš„å¹³è¡¡ã€‚"

20. **`-ctv, --cache-type-v TYPE`**
    - Current: "Cache type for V | V ç¼“å­˜ç±»å‹"
    - README: "KV cache data type for V<br/>allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1<br/>(default: f16)<br/>(env: LLAMA_ARG_CACHE_TYPE_V)"
    - Target: "KV cache data type for V (env: LLAMA_ARG_CACHE_TYPE_V) | KV ç¼“å­˜ä¸­ V å€¼çš„æ•°æ®ç±»å‹ï¼Œå¯é€‰å€¼ä¸ K ç›¸åŒã€‚é€šå¸¸ä¸ K è®¾ç½®ç›¸åŒç±»å‹ä»¥ä¿æŒä¸€è‡´æ€§ï¼Œå¯æ ¹æ®ç²¾åº¦éœ€æ±‚é€‰æ‹©ã€‚"

---

## Chunk 3: Common Parameters (Part 3) - Advanced Configuration
**Status: [ ] Pending**

1. **`-dt, --defrag-thold N`**
   - Current: "Defragmentation threshold | KV ç¼“å­˜ç¢ç‰‡æ•´ç†é˜ˆå€¼"
   - README: "KV cache defragmentation threshold (DEPRECATED)<br/>(env: LLAMA_ARG_DEFRAG_THOLD)"
   - Target: "KV cache defragmentation threshold (DEPRECATED) (env: LLAMA_ARG_DEFRAG_THOLD) | KV ç¼“å­˜ç¢ç‰‡æ•´ç†é˜ˆå€¼ã€‚å½“ç¢ç‰‡è¶…è¿‡æ­¤é˜ˆå€¼æ—¶è§¦å‘ç¢ç‰‡æ•´ç†ï¼Œæé«˜å†…å­˜åˆ©ç”¨ç‡ã€‚DEPRECATED å‚æ•°ï¼Œå¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­è¢«ç§»é™¤ã€‚"

2. **`-np, --parallel N`**
   - Current: "Number of parallel processes | å¹¶è¡Œè¿›ç¨‹æ•°é‡"
   - README: "number of parallel sequences to decode (default: 1)<br/>(env: LLAMA_ARG_N_PARALLEL)"
   - Target: "Number of parallel sequences to decode (default: 1) (env: LLAMA_ARG_N_PARALLEL) | å¹¶è¡Œè§£ç çš„åºåˆ—æ•°é‡ï¼Œæ”¯æŒå¤šç”¨æˆ·åŒæ—¶ä½¿ç”¨ã€‚è¾ƒé«˜çš„å€¼å¯æé«˜å¹¶å‘æ€§èƒ½ï¼Œä½†å¢åŠ å†…å­˜å’Œè®¡ç®—èµ„æºéœ€æ±‚ã€‚"

3. **`--mlock`**
   - Current: "Lock model in memory | å†…å­˜é”å®šæ¨¡å‹"
   - README: "force system to keep model in RAM rather than swapping or compressing<br/>(env: LLAMA_ARG_MLOCK)"
   - Target: "Force system to keep model in RAM rather than swapping or compressing (env: LLAMA_ARG_MLOCK) | å¼ºåˆ¶ç³»ç»Ÿå°†æ¨¡å‹ä¿æŒåœ¨ RAM ä¸­è€Œéäº¤æ¢æˆ–å‹ç¼©ï¼Œæé«˜æ€§èƒ½ä½†å¢åŠ å†…å­˜å ç”¨ã€‚é€‚ç”¨äºå†…å­˜å……è¶³ä¸”éœ€è¦ç¨³å®šæ€§èƒ½çš„åœºæ™¯ã€‚"

4. **`--no-mmap`**
   - Current: "Disable memory mapping | ç¦ç”¨å†…å­˜æ˜ å°„"
   - README: "do not memory-map model (slower load but may reduce pageouts if not using mlock)<br/>(env: LLAMA_ARG_NO_MMAP)"
   - Target: "Do not memory-map model (slower load but may reduce pageouts if not using mlock) (env: LLAMA_ARG_NO_MMAP) | ä¸ä½¿ç”¨å†…å­˜æ˜ å°„åŠ è½½æ¨¡å‹ï¼ŒåŠ è½½é€Ÿåº¦è¾ƒæ…¢ä½†å¯èƒ½å‡å°‘é¡µé¢äº¤æ¢ã€‚åœ¨ä¸ä½¿ç”¨ mlock æ—¶å¯ç”¨äºä¼˜åŒ–å†…å­˜ç®¡ç†ã€‚"

5. **`--numa TYPE`**
   - Current: "NUMA configuration | NUMA é…ç½®"
   - README: "attempt optimizations that help on some NUMA systems<br/>- distribute: spread execution evenly over all nodes<br/>- isolate: only spawn threads on CPUs on the node that execution started on<br/>- numactl: use the CPU map provided by numactl<br/>if run without this previously, it is recommended to drop the system page cache before using this<br/>see https://github.com/ggml-org/llama.cpp/issues/1437<br/>(env: LLAMA_ARG_NUMA)"
   - Target: "Attempt optimizations that help on some NUMA systems (env: LLAMA_ARG_NUMA) | NUMA é…ç½®ã€‚å°è¯•åœ¨æŸäº› NUMA ç³»ç»Ÿä¸Šè¿›è¡Œä¼˜åŒ–ï¼šdistributeï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰ã€isolateï¼ˆä»…é™èµ·å§‹èŠ‚ç‚¹ï¼‰ã€numactlï¼ˆä½¿ç”¨ numactl æä¾›çš„ CPU æ˜ å°„ï¼‰ã€‚é€‚åˆå¤š CPU æ¶æ„ä¼˜åŒ–ã€‚"

6. **`-dev, --device <dev1,dev2,..>`**
   - Current: "Device to use | ä½¿ç”¨çš„è®¾å¤‡"
   - README: "comma-separated list of devices to use for offloading (none = don't offload)<br/>use --list-devices to see a list of available devices<br/>(env: LLAMA_ARG_DEVICE)"
   - Target: "Comma-separated list of devices to use for offloading (none = don't offload) (env: LLAMA_ARG_DEVICE) | ä½¿ç”¨çš„è®¾å¤‡ã€‚é€—å·åˆ†éš”çš„è®¾å¤‡åˆ—è¡¨ç”¨äºå¸è½½ï¼ˆnone è¡¨ç¤ºä¸å¸è½½ï¼‰ã€‚ä½¿ç”¨ --list-devices æŸ¥çœ‹å¯ç”¨è®¾å¤‡åˆ—è¡¨ã€‚æ”¯æŒ CPUã€GPU ç­‰å¤šç§è®¾å¤‡ç±»å‹ã€‚"

7. **`--list-devices`**
   - Current: "Print list of available devices and exit"
   - README: "print list of available devices and exit"
   - Target: "Print list of available devices and exit | æ‰“å°å¯ç”¨è®¾å¤‡åˆ—è¡¨å¹¶é€€å‡ºã€‚åˆ—å‡ºç³»ç»Ÿä¸­æ‰€æœ‰å¯ç”¨äºæ¨¡å‹æ¨ç†çš„è®¾å¤‡ï¼ˆCPUã€GPU ç­‰ï¼‰ï¼ŒåŒ…æ‹¬è®¾å¤‡ç±»å‹ã€å†…å­˜ä¿¡æ¯å’Œæ”¯æŒçš„åŠŸèƒ½ã€‚å¸®åŠ©ç”¨æˆ·äº†è§£å¯ç”¨çš„ç¡¬ä»¶èµ„æºå’Œè¿›è¡Œè®¾å¤‡é€‰æ‹©é…ç½®ã€‚"

8. **`--override-tensor, -ot <tensor name pattern>=<buffer type>,...`**
   - Current: "Override tensor buffer type"
   - README: "override tensor buffer type"
   - Target: "Override tensor buffer type | è¦†ç›–å¼ é‡ç¼“å†²åŒºç±»å‹ã€‚å…è®¸ä¸ºå¼ é‡æŒ‡å®šä¸åŒçš„ç¼“å†²åŒºç±»å‹ï¼Œæ ¼å¼ä¸º <å¼ é‡åç§°æ¨¡å¼>=<ç¼“å†²åŒºç±»å‹>ã€‚å¯ç”¨äºä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ï¼Œæˆ–è§£å†³å…¼å®¹æ€§é—®é¢˜ã€‚"

9. **`--cpu-moe, -cmoe`**
   - Current: "CPU MoE | CPU æ··åˆä¸“å®¶"
   - README: "keep all Mixture of Experts (MoE) weights in the CPU<br/>(env: LLAMA_ARG_CPU_MOE)"
   - Target: "Keep all Mixture of Experts (MoE) weights in the CPU (env: LLAMA_ARG_CPU_MOE) | CPU æ··åˆä¸“å®¶ã€‚å°†æ‰€æœ‰æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æƒé‡ä¿æŒåœ¨ CPU ä¸­ã€‚å¯¹äº MoE æ¨¡å‹ï¼Œå¼ºåˆ¶æ‰€æœ‰ä¸“å®¶ç½‘ç»œåœ¨ CPU ä¸Šè¿è¡Œï¼Œå¯èƒ½å½±å“æ€§èƒ½ä½†æé«˜å…¼å®¹æ€§ã€‚"

10. **`--n-cpu-moe, -ncmoe N`**
    - Current: "Number of CPU MoE | CPU MoE æ•°é‡"
    - README: "keep the Mixture of Experts (MoE) weights of the first N layers in the CPU<br/>(env: LLAMA_ARG_N_CPU_MOE)"
    - Target: "Keep the Mixture of Experts (MoE) weights of the first N layers in the CPU (env: LLAMA_ARG_N_CPU_MOE) | CPU MoE æ•°é‡ã€‚å°†å‰ N å±‚çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æƒé‡ä¿æŒåœ¨ CPU ä¸­ã€‚æä¾›æ›´ç»†ç²’åº¦çš„ MoE æ§åˆ¶ï¼Œå¹³è¡¡æ€§èƒ½å’Œå…¼å®¹æ€§éœ€æ±‚ã€‚"

11. **`-ngl, --gpu-layers, --n-gpu-layers N`**
    - Current: "Number of GPU layers | GPU å±‚æ•°"
    - README: "number of layers to store in VRAM<br/>(env: LLAMA_ARG_N_GPU_LAYERS)"
    - Target: "Number of layers to store in VRAM (env: LLAMA_ARG_N_GPU_LAYERS) | GPU å±‚æ•°ã€‚æŒ‡å®šå¸è½½åˆ° VRAM çš„ç¥ç»ç½‘ç»œå±‚æ•°ï¼Œå¯æ˜¾è‘—åŠ é€Ÿæ¨ç†ã€‚0 è¡¨ç¤ºä»…ä½¿ç”¨ CPUã€‚è®¾ç½®ä¸º -1 æˆ–è¶³å¤Ÿå¤§çš„æ•°å­—å¯å°†æ•´ä¸ªæ¨¡å‹ç§»è‡³ GPUã€‚"

12. **`-sm, --split-mode {none,layer,row}`**
    - Current: "Split mode for tensors | å¼ é‡åˆ†å‰²æ¨¡å¼"
    - README: "how to split the model across multiple GPUs, one of:<br/>- none: use one GPU only<br/>- layer (default): split layers and KV across GPUs<br/>- row: split rows across GPUs<br/>(env: LLAMA_ARG_SPLIT_MODE)"
    - Target: "How to split the model across multiple GPUs (env: LLAMA_ARG_SPLIT_MODE) | å¼ é‡åˆ†å‰²æ¨¡å¼ã€‚æ§åˆ¶å¦‚ä½•åœ¨å¤šä¸ª GPU é—´åˆ†å‰²æ¨¡å‹ï¼šnoneï¼ˆå• GPUï¼‰ã€layerï¼ˆé»˜è®¤ï¼Œå±‚åˆ†å‰²ï¼‰ã€rowï¼ˆè¡Œåˆ†å‰²ï¼‰ã€‚å½±å“å¤š GPU æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨ã€‚"

13. **`-ts, --tensor-split N0,N1,N2,...`**
    - Current: "Tensor split configuration | å¼ é‡åˆ†å‰²é…ç½®"
    - README: "fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1<br/>(env: LLAMA_ARG_TENSOR_SPLIT)"
    - Target: "Fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1 (env: LLAMA_ARG_TENSOR_SPLIT) | å¼ é‡åˆ†å‰²é…ç½®ã€‚æŒ‡å®šå°†æ¨¡å‹åˆ†é…åˆ°å¤šä¸ª GPU çš„æ¯”ä¾‹ï¼Œé€—å·åˆ†éš”çš„æ¯”ä¾‹åˆ—è¡¨ã€‚ä¾‹å¦‚ '3,1' è¡¨ç¤º GPU 0 åˆ†é… 75%ï¼ŒGPU 1 åˆ†é… 25%ã€‚ç”¨äºå¤š GPU åˆ†å¸ƒå¼æ¨ç†ã€‚"

14. **`-mg, --main-gpu INDEX`**
    - Current: "Main GPU to use | ä¸» GPU è®¾å¤‡"
    - README: "the GPU to use for the model (with split-mode = none), or for intermediate results and KV (with split-mode = row) (default: 0)<br/>(env: LLAMA_ARG_MAIN_GPU)"
    - Target: "The GPU to use for the model (with split-mode = none), or for intermediate results and KV (with split-mode = row) (default: 0) (env: LLAMA_ARG_MAIN_GPU) | ä¸» GPU è®¾å¤‡ã€‚å½“ split-mode ä¸º none æ—¶ä½¿ç”¨çš„ GPUï¼Œæˆ–å½“ split-mode ä¸º row æ—¶å­˜å‚¨ä¸­é—´ç»“æœå’Œ KV ç¼“å­˜çš„ GPUã€‚é»˜è®¤ä¸º 0ã€‚å¤š GPU ç³»ç»Ÿä¸­ç”¨äºè´Ÿè½½å‡è¡¡ã€‚"

15. **`--check-tensors`**
    - Current: "Check model tensor data for invalid values"
    - README: "check model tensor data for invalid values (default: false)"
    - Target: "Check model tensor data for invalid values (default: false) | æ£€æŸ¥æ¨¡å‹å¼ é‡æ•°æ®ä¸­çš„æ— æ•ˆå€¼ã€‚ç”¨äºæ¨¡å‹éªŒè¯å’Œè°ƒè¯•ï¼Œå¸®åŠ©è¯†åˆ«æŸåçš„æ¨¡å‹æ–‡ä»¶æˆ–å†…å­˜é—®é¢˜ã€‚"

16. **`--override-kv KEY=TYPE:VALUE`**
    - Current: "Override KV cache | è¦†ç›– KV ç¼“å­˜"
    - README: "advanced option to override model metadata by key. may be specified multiple times.<br/>types: int, float, bool, str. example: --override-kv tokenizer.ggml.add_bos_token=bool:false"
    - Target: "Advanced option to override model metadata by key. May be specified multiple times. Types: int, float, bool, str. Example: --override-kv tokenizer.ggml.add_bos_token=bool:false | è¦†ç›–æ¨¡å‹å…ƒæ•°æ®ã€‚é«˜çº§é€‰é¡¹ï¼ŒæŒ‰é”®è¦†ç›–æ¨¡å‹å…ƒæ•°æ®ã€‚å¯å¤šæ¬¡æŒ‡å®šã€‚ç”¨äºè‡ªå®šä¹‰æ¨¡å‹è¡Œä¸ºå’Œé…ç½®ã€‚"

17. **`--no-op-offload`**
    - Current: "Disable operator offload | ç¦ç”¨æ“ä½œç¬¦å¸è½½"
    - README: "disable offloading host tensor operations to device (default: false)"
    - Target: "Disable offloading host tensor operations to device (default: false) | ç¦ç”¨æ“ä½œç¬¦å¸è½½ã€‚ç¦ç”¨å°†ä¸»æœºå¼ é‡æ“ä½œå¸è½½åˆ°è®¾å¤‡çš„åŠŸèƒ½ã€‚åœ¨è®¾å¤‡å†…å­˜æœ‰é™æˆ–é‡åˆ°å…¼å®¹æ€§é—®é¢˜æ—¶ä½¿ç”¨ã€‚å¯èƒ½å½±å“æ€§èƒ½ï¼Œä½†æé«˜ç¨³å®šæ€§ã€‚"

18. **`--lora FNAME`**
    - Current: "LoRA adapters | LoRA é€‚é…å™¨"
    - README: "path to LoRA adapter (can be repeated to use multiple adapters)"
    - Target: "Path to LoRA adapter (can be repeated to use multiple adapters) | LoRA é€‚é…å™¨ã€‚LoRAï¼ˆLow-Rank Adaptationï¼‰é€‚é…å™¨è·¯å¾„ï¼Œå¯ç”¨äºå¤šæ¬¡æŒ‡å®šä»¥ä½¿ç”¨å¤šä¸ªé€‚é…å™¨ã€‚ç”¨äºæ¨¡å‹å¾®è°ƒå’Œä¸ªæ€§åŒ–ï¼Œåœ¨ä¸ä¿®æ”¹åŸæ¨¡å‹çš„æƒ…å†µä¸‹è°ƒæ•´æ¨¡å‹è¡Œä¸ºã€‚"

19. **`--lora-scaled FNAME SCALE`**
    - Current: "Scaled LoRA adapters | ç¼©æ”¾ LoRA é€‚é…å™¨"
    - README: "path to LoRA adapter with user defined scaling (can be repeated to use multiple adapters)"
    - Target: "Path to LoRA adapter with user defined scaling (can be repeated to use multiple adapters) | ç¼©æ”¾ LoRA é€‚é…å™¨ã€‚å¸¦æœ‰ç”¨æˆ·å®šä¹‰ç¼©æ”¾å› å­çš„ LoRA é€‚é…å™¨ï¼Œæ ¼å¼ä¸ºè·¯å¾„:ç¼©æ”¾å€¼ã€‚å¯ç”¨äºå¤šæ¬¡æŒ‡å®šä»¥ä½¿ç”¨å¤šä¸ªç¼©æ”¾é€‚é…å™¨ã€‚ç²¾ç¡®æ§åˆ¶æ¯ä¸ªé€‚é…å™¨çš„å½±å“ç¨‹åº¦ã€‚"

20. **`--control-vector FNAME`**
    - Current: "Control vectors | æ§åˆ¶å‘é‡"
    - README: "add a control vector<br/>note: this argument can be repeated to add multiple control vectors"
    - Target: "Add a control vector (can be repeated to add multiple control vectors) | æ§åˆ¶å‘é‡ã€‚æ·»åŠ æ§åˆ¶å‘é‡ä»¥è°ƒæ•´æ¨¡å‹è¡Œä¸ºã€‚å¯é‡å¤æ·»åŠ å¤šä¸ªæ§åˆ¶å‘é‡ã€‚ç”¨äºå¼•å¯¼æ¨¡å‹ç”Ÿæˆç‰¹å®šé£æ ¼ã€å†…å®¹æˆ–ä¸»é¢˜çš„æ–‡æœ¬ï¼Œå®ç°æ›´ç²¾ç»†çš„è¾“å‡ºæ§åˆ¶ã€‚"

---

## Chunk 4: Sampling Parameters
**Status: [ ] Pending**

1. **`--control-vector-scaled FNAME SCALE`**
   - Current: "Scaled control vectors | ç¼©æ”¾æ§åˆ¶å‘é‡"
   - README: "add a control vector with user defined scaling SCALE<br/>note: this argument can be repeated to add multiple scaled control vectors"
   - Target: "Add a control vector with user defined scaling SCALE (can be repeated to add multiple scaled control vectors) | ç¼©æ”¾æ§åˆ¶å‘é‡ã€‚æ·»åŠ å¸¦æœ‰ç”¨æˆ·å®šä¹‰ç¼©æ”¾å› å­çš„æ§åˆ¶å‘é‡ï¼Œæ ¼å¼ä¸ºè·¯å¾„:ç¼©æ”¾å€¼ã€‚ç²¾ç¡®æ§åˆ¶æ¯ä¸ªå‘é‡å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ç¨‹åº¦ã€‚"

2. **`--control-vector-layer-range START END`**
   - Current: "Control vector layer range | æ§åˆ¶å‘é‡å±‚èŒƒå›´"
   - README: "layer range to apply the control vector(s) to, start and end inclusive"
   - Target: "Layer range to apply the control vector(s) to, start and end inclusive | æ§åˆ¶å‘é‡å±‚èŒƒå›´ã€‚åº”ç”¨æ§åˆ¶å‘é‡çš„å±‚èŒƒå›´ï¼Œå¼€å§‹å’Œç»“æŸå±‚éƒ½åŒ…å«åœ¨å†…ã€‚æŒ‡å®šæ§åˆ¶å‘é‡å½±å“çš„å…·ä½“ç¥ç»ç½‘ç»œå±‚ï¼Œå®ç°æ›´ç²¾å‡†çš„æ¨¡å‹è¡Œä¸ºè°ƒæ•´ã€‚"

3. **`-m, --model FNAME`**
   - Current: "Model file path | æ¨¡å‹æ–‡ä»¶è·¯å¾„"
   - README: "model path (default: `models/$filename` with filename from `--hf-file` or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)<br/>(env: LLAMA_ARG_MODEL)"
   - Target: "Model path (default: `models/$filename` with filename from `--hf-file` or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf) (env: LLAMA_ARG_MODEL) | æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚æŒ‡å®šè¦åŠ è½½çš„ GGUF æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶è·¯å¾„æˆ–ç½‘ç»œ URLã€‚å¦‚æœä½¿ç”¨ --hf-repo æˆ– --model-urlï¼Œæ­¤å‚æ•°å¯é€‰ã€‚"

4. **`-mu, --model-url MODEL_URL`**
   - Current: "Model URL | æ¨¡å‹ URL"
   - README: "model download url (default: unused)<br/>(env: LLAMA_ARG_MODEL_URL)"
   - Target: "Model download url (default: unused) (env: LLAMA_ARG_MODEL_URL) | æ¨¡å‹ä¸‹è½½ URLã€‚æ”¯æŒä»ç½‘ç»œç›´æ¥ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚æä¾›å¦ä¸€ç§æ¨¡å‹è·å–æ–¹å¼ï¼Œæ— éœ€æ‰‹åŠ¨ä¸‹è½½ã€‚ç¡®ä¿ URL å¯è®¿é—®ä¸”æ¨¡å‹æ ¼å¼å…¼å®¹ã€‚"

5. **`-hf, -hfr, --hf-repo <user>/<model>[:quant]`**
   - Current: "Hugging Face repository | Hugging Face ä»“åº“"
   - README: "Hugging Face model repository; quant is optional, case-insensitive, default to Q4_K_M, or falls back to the first file in the repo if Q4_K_M doesn't exist.<br/>mmproj is also downloaded automatically if available. to disable, add --no-mmproj<br/>example: unsloth/phi-4-GGUF:q4_k_m<br/>(default: unused)<br/>(env: LLAMA_ARG_HF_REPO)"
   - Target: "Hugging Face model repository; quant is optional, case-insensitive, default to Q4_K_M, or falls back to the first file in the repo if Q4_K_M doesn't exist. mmproj is also downloaded automatically if available. to disable, add --no-mmproj. Example: unsloth/phi-4-GGUF:q4_k_m (env: LLAMA_ARG_HF_REPO) | Hugging Face æ¨¡å‹ä»“åº“ã€‚Hugging Face æ¨¡å‹ä»“åº“ï¼Œæ ¼å¼ä¸º <user>/<model>[:quant]ã€‚quant å¯é€‰ï¼Œä¸åŒºåˆ†å¤§å°å†™ï¼Œé»˜è®¤ä¸º Q4_K_Mã€‚ç¤ºä¾‹ï¼šunsloth/phi-4-GGUF:q4_k_m"

6. **`-hfd, -hfrd, --hf-repo-draft <user>/<model>[:quant]`**
   - Current: "Hugging Face draft repository | Hugging Face è‰ç¨¿ä»“åº“"
   - README: "Same as --hf-repo, but for the draft model (default: unused)<br/>(env: LLAMA_ARG_HFD_REPO)"
   - Target: "Same as --hf-repo, but for the draft model (default: unused) (env: LLAMA_ARG_HFD_REPO) | Hugging Face è‰ç¨¿ä»“åº“ã€‚ç”¨äºæŠ•æœºè§£ç çš„è‰ç¨¿æ¨¡å‹ä»“åº“ï¼Œæ ¼å¼ä¸ --hf-repo ç›¸åŒã€‚è‰ç¨¿æ¨¡å‹é€šå¸¸è¾ƒå°ï¼Œç”¨äºå¿«é€Ÿç”Ÿæˆå€™é€‰åºåˆ—ï¼Œæé«˜æ•´ä½“æ¨ç†é€Ÿåº¦ã€‚"

7. **`-hff, --hf-file FILE`**
   - Current: "Hugging Face file | Hugging Face æ–‡ä»¶"
   - README: "Hugging Face model file. If specified, it will override the quant in --hf-repo (default: unused)<br/>(env: LLAMA_ARG_HF_FILE)"
   - Target: "Hugging Face model file. If specified, it will override the quant in --hf-repo (default: unused) (env: LLAMA_ARG_HF_FILE) | Hugging Face æ–‡ä»¶ã€‚Hugging Face æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚æœæŒ‡å®šå°†è¦†ç›– --hf-repo ä¸­çš„ quantã€‚ç”¨äºç²¾ç¡®æŒ‡å®šè¦ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ”¯æŒç‰¹å®šçš„é‡åŒ–ç‰ˆæœ¬ã€‚"

8. **`-hfv, -hfrv, --hf-repo-v <user>/<model>[:quant]`**
   - Current: "Hugging Face repository for vision | è§†è§‰æ¨¡å‹ä»“åº“"
   - README: "Hugging Face model repository for the vocoder model (default: unused)<br/>(env: LLAMA_ARG_HF_REPO_V)"
   - Target: "Hugging Face model repository for the vocoder model (default: unused) (env: LLAMA_ARG_HF_REPO_V) | è§†è§‰æ¨¡å‹ä»“åº“ã€‚ç”¨äºå¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç¼–ç å™¨ä»“åº“ï¼Œæ”¯æŒå›¾åƒç†è§£å’Œå¤šæ¨¡æ€æ¨ç†ã€‚ä¸ä¸»æ¨¡å‹é…åˆä½¿ç”¨ï¼Œå®ç°å›¾åƒ-æ–‡æœ¬è”åˆå¤„ç†èƒ½åŠ›ã€‚"

9. **`-hffv, --hf-file-v FILE`**
   - Current: "Hugging Face file for vision | è§†è§‰æ¨¡å‹æ–‡ä»¶"
   - README: "Hugging Face model file for the vocoder model (default: unused)<br/>(env: LLAMA_ARG_HF_FILE_V)"
   - Target: "Hugging Face model file for the vocoder model (default: unused) (env: LLAMA_ARG_HF_FILE_V) | è§†è§‰æ¨¡å‹æ–‡ä»¶ã€‚å¤šæ¨¡æ€è§†è§‰ç¼–ç å™¨çš„å…·ä½“æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºç²¾ç¡®æŒ‡å®šè§†è§‰ç»„ä»¶ã€‚ç¡®ä¿ä¸è¯­è¨€æ¨¡å‹ç‰ˆæœ¬å…¼å®¹ï¼Œæ”¯æŒé«˜è´¨é‡çš„å›¾åƒç†è§£åŠŸèƒ½ã€‚"

10. **`-hft, --hf-token TOKEN`**
    - Current: "Hugging Face token | Hugging Face ä»¤ç‰Œ"
    - README: "Hugging Face access token (default: value from HF_TOKEN environment variable)<br/>(env: HF_TOKEN)"
    - Target: "Hugging Face access token (default: value from HF_TOKEN environment variable) (env: HF_TOKEN) | Hugging Face ä»¤ç‰Œã€‚Hugging Face è®¿é—®ä»¤ç‰Œï¼Œç”¨äºè®¿é—®ç§æœ‰æ¨¡å‹ä»“åº“æˆ–éœ€è¦è®¤è¯çš„æ¨¡å‹ã€‚é»˜è®¤ä» HF_TOKEN ç¯å¢ƒå˜é‡è·å–ã€‚ä¿æŠ¤æ¨¡å‹è®¿é—®æƒé™çš„é‡è¦å®‰å…¨æªæ–½ã€‚"

11. **`--log-disable`**
    - Current: "Disable logging | ç¦ç”¨æ—¥å¿—è®°å½•"
    - README: "Log disable"
    - Target: "Log disable | ç¦ç”¨æ—¥å¿—è®°å½•ã€‚å®Œå…¨ç¦ç”¨æ‰€æœ‰æ—¥å¿—è¾“å‡ºï¼Œå‡å°‘æ§åˆ¶å°å™ªéŸ³å’Œæé«˜æ€§èƒ½ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯èƒ½æœ‰ç”¨ï¼Œä½†ä¼šå½±å“é—®é¢˜è¯Šæ–­å’Œç›‘æ§èƒ½åŠ›ã€‚"

12. **`--log-file FNAME`**
    - Current: "Log file path | æ—¥å¿—æ–‡ä»¶è·¯å¾„"
    - README: "Log to file"
    - Target: "Log to file | æ—¥å¿—æ–‡ä»¶è·¯å¾„ã€‚å°†æ—¥å¿—è¾“å‡ºé‡å®šå‘åˆ°æŒ‡å®šæ–‡ä»¶ï¼Œä¾¿äºé•¿æœŸä¿å­˜å’Œåˆ†ææ—¥å¿—ã€‚æ”¯æŒæ—¥å¿—è½®è½¬å’Œå½’æ¡£ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å’Œé—®é¢˜è¿½è¸ªã€‚"

13. **`--log-colors`**
    - Current: "Enable log colors | å¯ç”¨æ—¥å¿—é¢œè‰²"
    - README: "Enable colored logging<br/>(env: LLAMA_LOG_COLORS)"
    - Target: "Enable colored logging (env: LLAMA_LOG_COLORS) | å¯ç”¨æ—¥å¿—é¢œè‰²ã€‚ä¸ºä¸åŒçº§åˆ«çš„æ—¥å¿—æ¶ˆæ¯æ·»åŠ é¢œè‰²ï¼Œæé«˜å¯è¯»æ€§å’Œå¿«é€Ÿè¯†åˆ«é—®é¢˜ã€‚åœ¨æ”¯æŒé¢œè‰²çš„ç»ˆç«¯ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œä¾¿äºå¿«é€Ÿå®šä½é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯ã€‚"

14. **`-v, --verbose, --log-verbose`**
    - Current: "Verbose logging | è¯¦ç»†æ—¥å¿—è®°å½•"
    - README: "Set verbosity level to infinity (i.e. log all messages, useful for debugging)"
    - Target: "Set verbosity level to infinity (i.e. log all messages, useful for debugging) | è¯¦ç»†æ—¥å¿—è®°å½•ã€‚å°†è¯¦ç»†çº§åˆ«è®¾ç½®ä¸ºæ— é™ï¼ˆå³è®°å½•æ‰€æœ‰æ¶ˆæ¯ï¼‰ï¼Œé€‚ç”¨äºè°ƒè¯•ã€‚æä¾›æœ€è¯¦ç»†çš„å†…éƒ¨çŠ¶æ€ä¿¡æ¯ï¼Œå¸®åŠ©è¯Šæ–­å¤æ‚é—®é¢˜ã€‚"

15. **`--offline`**
    - Current: "Offline mode | ç¦»çº¿æ¨¡å¼"
    - README: "Offline mode: forces use of cache, prevents network access<br/>(env: LLAMA_OFFLINE)"
    - Target: "Offline mode: forces use of cache, prevents network access (env: LLAMA_OFFLINE) | ç¦»çº¿æ¨¡å¼ã€‚å¼ºåˆ¶ä½¿ç”¨ç¼“å­˜ï¼Œé˜²æ­¢ç½‘ç»œè®¿é—®ã€‚é€‚ç”¨äºç½‘ç»œå—é™ç¯å¢ƒæˆ–ç¡®ä¿å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œé¿å…æ„å¤–ä¸‹è½½æˆ–å¤–éƒ¨ä¾èµ–ã€‚"

16. **`-lv, --verbosity, --log-verbosity N`**
    - Current: "Verbosity level | è¯¦ç»†çº§åˆ«"
    - README: "Set the verbosity threshold. Messages with a higher verbosity will be ignored.<br/>(env: LLAMA_LOG_VERBOSITY)"
    - Target: "Set the verbosity threshold. Messages with a higher verbosity will be ignored. (env: LLAMA_LOG_VERBOSITY) | è¯¦ç»†çº§åˆ«ã€‚è®¾ç½®è¯¦ç»†ç¨‹åº¦é˜ˆå€¼ï¼Œé«˜äºæ­¤çº§åˆ«çš„æ¶ˆæ¯å°†è¢«å¿½ç•¥ã€‚æä¾›ç²¾ç»†çš„æ—¥å¿—çº§åˆ«æ§åˆ¶ï¼Œå¹³è¡¡ä¿¡æ¯é‡å’Œæ€§èƒ½å½±å“ã€‚"

17. **`--log-prefix`**
    - Current: "Log prefix | æ—¥å¿—å‰ç¼€"
    - README: "Enable prefix in log messages<br/>(env: LLAMA_LOG_PREFIX)"
    - Target: "Enable prefix in log messages (env: LLAMA_LOG_PREFIX) | æ—¥å¿—å‰ç¼€ã€‚åœ¨æ—¥å¿—æ¶ˆæ¯ä¸­å¯ç”¨å‰ç¼€ï¼ŒåŒ…å«æ—¶é—´æˆ³ã€çº§åˆ«ç­‰ä¿¡æ¯ã€‚æé«˜æ—¥å¿—çš„ç»“æ„åŒ–å’Œå¯è¯»æ€§ï¼Œä¾¿äºæ—¥å¿—åˆ†æå’Œè¿‡æ»¤ã€‚"

18. **`--log-timestamps`**
    - Current: "Log timestamps | æ—¥å¿—æ—¶é—´æˆ³"
    - README: "Enable timestamps in log messages<br/>(env: LLAMA_LOG_TIMESTAMPS)"
    - Target: "Enable timestamps in log messages (env: LLAMA_LOG_TIMESTAMPS) | æ—¥å¿—æ—¶é—´æˆ³ã€‚åœ¨æ—¥å¿—æ¶ˆæ¯ä¸­å¯ç”¨æ—¶é—´æˆ³ï¼Œè®°å½•äº‹ä»¶å‘ç”Ÿçš„ç²¾ç¡®æ—¶é—´ã€‚å¯¹äºæ€§èƒ½åˆ†æã€é—®é¢˜è¿½è¸ªå’Œå®¡è®¡æ—¥å¿—éå¸¸é‡è¦ã€‚"

19. **`-ctkd, --cache-type-k-draft TYPE`**
    - Current: "Cache type for K for the draft model"
    - README: "KV cache data type for K for the draft model<br/>allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1<br/>(default: f16)<br/>(env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)"
    - Target: "KV cache data type for K for the draft model (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT) | è‰ç¨¿æ¨¡å‹çš„ KV ç¼“å­˜ K é”®æ•°æ®ç±»å‹ã€‚å…è®¸å€¼ï¼šf32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1ï¼ˆé»˜è®¤ï¼šf16ï¼‰ã€‚è‰ç¨¿æ¨¡å‹çš„ KV ç¼“å­˜æ•°æ®ç±»å‹å¯ä»¥ä¸ä¸»æ¨¡å‹ä¸åŒï¼Œå…è®¸åœ¨ç²¾åº¦å’Œå†…å­˜æ•ˆç‡é—´è¿›è¡Œç‹¬ç«‹ä¼˜åŒ–ã€‚"

20. **`-ctvd, --cache-type-v-draft TYPE`**
    - Current: "Cache type for V for the draft model"
    - README: "KV cache data type for V for the draft model<br/>allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1<br/>(default: f16)<br/>(env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)"
    - Target: "KV cache data type for V for the draft model (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT) | è‰ç¨¿æ¨¡å‹çš„ KV ç¼“å­˜ V å€¼æ•°æ®ç±»å‹ã€‚å…è®¸å€¼ï¼šf32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1ï¼ˆé»˜è®¤ï¼šf16ï¼‰ã€‚é€šå¸¸ä¸ K è®¾ç½®ç›¸åŒç±»å‹ä»¥ä¿æŒä¸€è‡´æ€§ï¼Œä½†å¯ä»¥æ ¹æ®è‰ç¨¿æ¨¡å‹çš„ç‰¹æ€§è¿›è¡Œç‹¬ç«‹é…ç½®ã€‚"

---

## Chunk 5: Example-Specific Parameters (Server & Network)
**Status: [ ] Pending**

1. **`--swa-checkpoints N`**
   - Current: "Max number of SWA checkpoints per slot to create"
   - README: "max number of SWA checkpoints per slot to create (default: 3)<br/>[(more info)](https://github.com/ggml-org/llama.cpp/pull/15293)<br/>(env: LLAMA_ARG_SWA_CHECKPOINTS)"
   - Target: "Max number of SWA checkpoints per slot to create (default: 3) (env: LLAMA_ARG_SWA_CHECKPOINTS) | æ¯ä¸ª slot åˆ›å»ºçš„æœ€å¤§ SWA æ£€æŸ¥ç‚¹æ•°ã€‚æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆSWAï¼‰æ£€æŸ¥ç‚¹ç”¨äºç®¡ç†é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„çŠ¶æ€ï¼Œæ›´å¤šçš„æ£€æŸ¥ç‚¹å¯ä»¥æé«˜é•¿æ–‡æœ¬å¤„ç†çš„è¿ç»­æ€§ä½†å¢åŠ å†…å­˜ä½¿ç”¨ã€‚"

2. **`--no-context-shift`**
   - Current: "Disables context shift on infinite text generation"
   - README: "disables context shift on infinite text generation (default: enabled)<br/>(env: LLAMA_ARG_NO_CONTEXT_SHIFT)"
   - Target: "Disables context shift on infinite text generation (default: enabled) (env: LLAMA_ARG_NO_CONTEXT_SHIFT) | ç¦ç”¨ä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚åœ¨æ— é™æ–‡æœ¬ç”Ÿæˆæ—¶ç¦ç”¨ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œé»˜è®¤å¯ç”¨ã€‚å¯èƒ½åœ¨æŸäº›ç‰¹å®šç”Ÿæˆä»»åŠ¡ä¸­éœ€è¦ä¿æŒä¸¥æ ¼çš„ä¸Šä¸‹æ–‡è¿ç»­æ€§ã€‚"

3. **`--context-shift`**
   - Current: "Enables context shift on infinite text generation"
   - README: "enables context shift on infinite text generation (default: disabled)<br/>(env: LLAMA_ARG_CONTEXT_SHIFT)"
   - Target: "Enables context shift on infinite text generation (default: disabled) (env: LLAMA_ARG_CONTEXT_SHIFT) | å¯ç”¨ä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚åœ¨æ— é™æ–‡æœ¬ç”Ÿæˆæ—¶å¯ç”¨ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼Œé»˜è®¤ç¦ç”¨ã€‚å…è®¸å¤„ç†è¶…å‡ºä¸Šä¸‹æ–‡çª—å£çš„é•¿æ–‡æœ¬ï¼Œä½†å¯èƒ½ä¸¢å¤±ä¸€äº›æ—©æœŸä¿¡æ¯ã€‚"

4. **`-r, --reverse-prompt PROMPT`**
   - Current: "Halt generation at PROMPT, return control in interactive mode"
   - README: "halt generation at PROMPT, return control in interactive mode<br/>"
   - Target: "Halt generation at PROMPT, return control in interactive mode | åœ¨æŒ‡å®šæç¤ºç¬¦å¤„åœæ­¢ç”Ÿæˆï¼Œåœ¨äº¤äº’æ¨¡å¼ä¸‹è¿”å›æ§åˆ¶æƒã€‚å½“æ¨¡å‹ç”Ÿæˆåˆ°æŒ‡å®šçš„æç¤ºå­—ç¬¦ä¸²æ—¶è‡ªåŠ¨åœæ­¢ï¼Œé€‚ç”¨äºå¯¹è¯ç³»ç»Ÿã€ä»£ç ç”Ÿæˆç­‰éœ€è¦ç²¾ç¡®æ§åˆ¶è¾“å‡ºé•¿åº¦çš„åœºæ™¯ã€‚"

5. **`-sp, --special`**
   - Current: "Special tokens output enabled"
   - README: "special tokens output enabled (default: false)"
   - Target: "Special tokens output enabled (default: false) | å¯ç”¨ç‰¹æ®Š token è¾“å‡ºã€‚æ§åˆ¶æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«ç‰¹æ®Š tokenï¼ˆå¦‚ BOSã€EOSã€PAD ç­‰ï¼‰ã€‚å¯¹äºéœ€è¦å®Œæ•´ token çº§åˆ«æ§åˆ¶çš„åœºæ™¯ï¼ˆå¦‚æ¨¡å‹è°ƒè¯•ã€æ•°æ®é¢„å¤„ç†ï¼‰å¾ˆæœ‰ç”¨ã€‚"

6. **`--no-warmup`**
   - Current: "Skip warming up the model with an empty run"
   - README: "skip warming up the model with an empty run"
   - Target: "Skip warming up the model with an empty run | è·³è¿‡æ¨¡å‹é¢„çƒ­ã€‚åœ¨å¼€å§‹æ­£å¼æ¨ç†å‰è·³è¿‡æ¨¡å‹é¢„çƒ­æ­¥éª¤ï¼Œå¯åŠ å¿«å¯åŠ¨é€Ÿåº¦ä½†å¯èƒ½å½±å“ç¬¬ä¸€æ¬¡æ¨ç†çš„æ€§èƒ½ã€‚é€‚ç”¨äºå¿«é€Ÿæµ‹è¯•å’Œå¼€å‘ç¯å¢ƒã€‚"

7. **`--spm-infill`**
   - Current: "Use Suffix/Prefix/Middle pattern for infill"
   - README: "use Suffix/Prefix/Middle pattern for infill (instead of Prefix/Suffix/Middle) as some models prefer this. (default: disabled)"
   - Target: "Use Suffix/Prefix/Middle pattern for infill (instead of Prefix/Suffix/Middle) as some models prefer this (default: disabled) | ä½¿ç”¨ Suffix/Prefix/Middle æ¨¡å¼è¿›è¡Œå¡«å……ã€‚ä½¿ç”¨ Suffix/Prefix/Middle æ¨¡å¼è¿›è¡Œå¡«å……ï¼ˆè€Œä¸æ˜¯ Prefix/Suffix/Middleï¼‰ï¼Œå› ä¸ºæŸäº›æ¨¡å‹æ›´å€¾å‘äºè¿™ç§æ¨¡å¼ã€‚é€‚ç”¨äºä»£ç è¡¥å…¨ã€æ–‡æœ¬ä¿®å¤ç­‰å¡«å……ä»»åŠ¡ã€‚"

8. **`--pooling {none,mean,cls,last,rank}`**
   - Current: "Pooling type for embeddings, use model default if unspecified"
   - README: "pooling type for embeddings, use model default if unspecified<br/>(env: LLAMA_ARG_POOLING)"
   - Target: "Pooling type for embeddings, use model default if unspecified (env: LLAMA_ARG_POOLING) | åµŒå…¥çš„æ± åŒ–ç±»å‹ã€‚æ± åŒ–ç±»å‹å½±å“å¦‚ä½•å°† token çº§åˆ«çš„åµŒå…¥èšåˆä¸ºå¥å­çº§åˆ«æˆ–æ–‡æ¡£çº§åˆ«çš„è¡¨ç¤ºã€‚å¯é€‰å€¼ï¼šnoneï¼ˆä¸æ± åŒ–ï¼‰ã€meanï¼ˆå¹³å‡å€¼æ± åŒ–ï¼‰ã€clsï¼ˆCLS token æ± åŒ–ï¼‰ã€lastï¼ˆæœ€åä¸€ä¸ª token æ± åŒ–ï¼‰ã€rankï¼ˆæ’åºæ± åŒ–ï¼‰ã€‚"

9. **`-cb, --cont-batching`**
   - Current: "Enable continuous batching"
   - README: "enable continuous batching (a.k.a dynamic batching) (default: enabled)<br/>(env: LLAMA_ARG_CONT_BATCHING)"
   - Target: "Enable continuous batching (a.k.a dynamic batching) (default: enabled) (env: LLAMA_ARG_CONT_BATCHING) | å¯ç”¨è¿ç»­æ‰¹å¤„ç†ã€‚å¯ç”¨è¿ç»­æ‰¹å¤„ç†ï¼ˆä¹Ÿç§°ä¸ºåŠ¨æ€æ‰¹å¤„ç†ï¼‰ï¼Œæé«˜å¤šç”¨æˆ·åœºæ™¯ä¸‹çš„ååé‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚é»˜è®¤å¯ç”¨ï¼Œæ˜¯ç°ä»£ LLM æœåŠ¡çš„æ ‡å‡†é…ç½®ã€‚"

10. **`-nocb, --no-cont-batching`**
    - Current: "Disable continuous batching"
    - README: "disable continuous batching<br/>(env: LLAMA_ARG_NO_CONT_BATCHING)"
    - Target: "Disable continuous batching (env: LLAMA_ARG_NO_CONT_BATCHING) | ç¦ç”¨è¿ç»­æ‰¹å¤„ç†ã€‚ç¦ç”¨è¿ç»­æ‰¹å¤„ç†ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ‰¹å¤„ç†æ¨¡å¼ã€‚å¯èƒ½åœ¨æŸäº›ç‰¹å®šåœºæ™¯ä¸‹éœ€è¦ï¼Œä½†é€šå¸¸ä¼šé™ä½æ•´ä½“æ€§èƒ½ã€‚"

11. **`--mmproj FILE`**
    - Current: "Path to a multimodal projector file. see tools/mtmd/README.md"
    - README: "path to a multimodal projector file. see tools/mtmd/README.md<br/>note: if -hf is used, this argument can be omitted<br/>(env: LLAMA_ARG_MMPROJ)"
    - Target: "Path to a multimodal projector file. see tools/mtmd/README.md (env: LLAMA_ARG_MMPROJ) | å¤šæ¨¡æ€æŠ•å½±æ–‡ä»¶è·¯å¾„ã€‚å¤šæ¨¡æ€æŠ•å½±æ–‡ä»¶ç”¨äºå¤„ç†å›¾åƒã€è§†é¢‘ç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆä¸è§†è§‰å†…å®¹ç›¸å…³çš„æ–‡æœ¬å“åº”ã€‚å¦‚æœä½¿ç”¨ -hf å‚æ•°ï¼Œæ­¤å‚æ•°å¯ä»¥çœç•¥ã€‚"

12. **`--mmproj-url URL`**
    - Current: "URL to a multimodal projector file. see tools/mtmd/README.md"
    - README: "URL to a multimodal projector file. see tools/mtmd/README.md<br/>(env: LLAMA_ARG_MMPROJ_URL)"
    - Target: "URL to a multimodal projector file. see tools/mtmd/README.md (env: LLAMA_ARG_MMPROJ_URL) | å¤šæ¨¡æ€æŠ•å½±æ–‡ä»¶ URLã€‚ä»ç½‘ç»œ URL åŠ è½½å¤šæ¨¡æ€æŠ•å½±æ–‡ä»¶ï¼Œæ”¯æŒè¿œç¨‹éƒ¨ç½²å’ŒåŠ¨æ€ä¸‹è½½å¤šæ¨¡æ€å¤„ç†ç»„ä»¶ã€‚ç¡®ä¿ URL å¯è®¿é—®ä¸”æ–‡ä»¶æ ¼å¼å…¼å®¹ã€‚"

13. **`--no-mmproj`**
    - Current: "Explicitly disable multimodal projector, useful when using -hf"
    - README: "explicitly disable multimodal projector, useful when using -hf<br/>(env: LLAMA_ARG_NO_MMPROJ)"
    - Target: "Explicitly disable multimodal projector, useful when using -hf (env: LLAMA_ARG_NO_MMPROJ) | æ˜¾å¼ç¦ç”¨å¤šæ¨¡æ€æŠ•å½±å™¨ã€‚å¼ºåˆ¶ç¦ç”¨å¤šæ¨¡æ€åŠŸèƒ½ï¼Œå³ä½¿åœ¨ä½¿ç”¨ Hugging Face æ¨¡å‹æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚é€‚ç”¨äºçº¯æ–‡æœ¬æ¨ç†åœºæ™¯ï¼Œé¿å…ä¸å¿…è¦çš„å¤šæ¨¡æ€ç»„ä»¶åŠ è½½ã€‚"

14. **`--no-mmproj-offload`**
    - Current: "Do not offload multimodal projector to GPU"
    - README: "do not offload multimodal projector to GPU<br/>(env: LLAMA_ARG_NO_MMPROJ_OFFLOAD)"
    - Target: "Do not offload multimodal projector to GPU (env: LLAMA_ARG_NO_MMPROJ_OFFLOAD) | ä¸å°†å¤šæ¨¡æ€æŠ•å½±å™¨å¸è½½åˆ° GPUã€‚å¼ºåˆ¶å¤šæ¨¡æ€æŠ•å½±å™¨åœ¨ CPU ä¸Šè¿è¡Œï¼Œé¿å… GPU å†…å­˜å ç”¨ã€‚åœ¨ GPU å†…å­˜æœ‰é™æˆ–é‡åˆ°å…¼å®¹æ€§é—®é¢˜æ—¶ä½¿ç”¨ã€‚"

15. **`--override-tensor-draft, -otd <tensor name pattern>=<buffer type>,...`**
    - Current: "Override tensor buffer type for draft model"
    - README: "override tensor buffer type for draft model"
    - Target: "Override tensor buffer type for draft model | è¦†ç›–è‰ç¨¿æ¨¡å‹çš„å¼ é‡ç¼“å†²åŒºç±»å‹ã€‚å…è®¸ä¸ºè‰ç¨¿æ¨¡å‹æŒ‡å®šä¸åŒçš„å¼ é‡ç¼“å†²åŒºç±»å‹ï¼Œæ ¼å¼ä¸º <å¼ é‡åç§°æ¨¡å¼>=<ç¼“å†²åŒºç±»å‹>ã€‚å¯ç”¨äºä¼˜åŒ–è‰ç¨¿æ¨¡å‹çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ï¼Œæˆ–è§£å†³å…¼å®¹æ€§é—®é¢˜ã€‚"

16. **`--cpu-moe-draft, -cmoed`**
    - Current: "CPU MoE draft"
    - README: "keep all Mixture of Experts (MoE) weights in the CPU for the draft model<br/>(env: LLAMA_ARG_CPU_MOE_DRAFT)"
    - Target: "Keep all Mixture of Experts (MoE) weights in the CPU for the draft model (env: LLAMA_ARG_CPU_MOE_DRAFT) | CPU MoE è‰ç¨¿ã€‚å¯¹äºè‰ç¨¿æ¨¡å‹ï¼Œå°†æ‰€æœ‰æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æƒé‡ä¿æŒåœ¨ CPU ä¸­ã€‚åœ¨æŠ•æœºè§£ç åœºæ™¯ä¸­æ§åˆ¶è‰ç¨¿æ¨¡å‹çš„ MoE è¡Œä¸ºã€‚"

17. **`--n-cpu-moe-draft, -ncmoed N`**
    - Current: "Number of CPU MoE draft"
    - README: "keep the Mixture of Experts (MoE) weights of the first N layers in the CPU for the draft model<br/>(env: LLAMA_ARG_N_CPU_MOE_DRAFT)"
    - Target: "Keep the Mixture of Experts (MoE) weights of the first N layers in the CPU for the draft model (env: LLAMA_ARG_N_CPU_MOE_DRAFT) | CPU MoE è‰ç¨¿æ•°é‡ã€‚å¯¹äºè‰ç¨¿æ¨¡å‹ï¼Œå°†å‰ N å±‚çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æƒé‡ä¿æŒåœ¨ CPU ä¸­ã€‚åœ¨æŠ•æœºè§£ç ä¸­ç²¾ç»†æ§åˆ¶è‰ç¨¿æ¨¡å‹çš„ MoE è¡Œä¸ºã€‚"

18. **`-a, --alias STRING`**
    - Current: "Set alias for model name (to be used by REST API)"
    - README: "set alias for model name (to be used by REST API)<br/>(env: LLAMA_ARG_ALIAS)"
    - Target: "Set alias for model name (to be used by REST API) (env: LLAMA_ARG_ALIAS) | ä¸ºæ¨¡å‹åç§°è®¾ç½®åˆ«åã€‚ä¸ºæ¨¡å‹æŒ‡å®šä¸€ä¸ªå‹å¥½çš„åˆ«åï¼Œåœ¨ API è°ƒç”¨ä¸­ä½¿ç”¨æ­¤åˆ«åè€Œä¸æ˜¯æ¨¡å‹æ–‡ä»¶åã€‚ä¾¿äºæ¨¡å‹ç®¡ç†å’Œç‰ˆæœ¬æ§åˆ¶ï¼Œæ”¯æŒåœ¨è¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢æ¨¡å‹ã€‚"

19. **`--host HOST`**
    - Current: "Server host"
    - README: "ip address to listen, or bind to an UNIX socket if the address ends with .sock (default: 127.0.0.1)<br/>(env: LLAMA_ARG_HOST)"
    - Target: "IP address to listen, or bind to an UNIX socket if the address ends with .sock (default: 127.0.0.1) (env: LLAMA_ARG_HOST) | æœåŠ¡å™¨ä¸»æœºã€‚æœåŠ¡å™¨ç›‘å¬çš„ IP åœ°å€ï¼Œæˆ–å¦‚æœåœ°å€ä»¥ .sock ç»“å°¾åˆ™ç»‘å®šåˆ° UNIX å¥—æ¥å­—ã€‚é»˜è®¤ä¸º 127.0.0.1ï¼ˆä»…æœ¬åœ°è®¿é—®ï¼‰ã€‚è®¾ç½®ä¸º 0.0.0.0 å…è®¸å¤–éƒ¨è®¿é—®ã€‚"

20. **`--port PORT`**
    - Current: "Server port"
    - README: "port to listen (default: 8080)<br/>(env: LLAMA_ARG_PORT)"
    - Target: "Port to listen (default: 8080) (env: LLAMA_ARG_PORT) | æœåŠ¡å™¨ç«¯å£ã€‚æœåŠ¡å™¨ç›‘å¬çš„ç«¯å£å·ï¼Œé»˜è®¤ä¸º 8080ã€‚ç¡®ä¿ç«¯å£æœªè¢«å…¶ä»–æœåŠ¡å ç”¨ï¼Œä¸”é˜²ç«å¢™é…ç½®å…è®¸è®¿é—®ã€‚æ”¯æŒ 1-65535 èŒƒå›´å†…çš„ç«¯å£å·ã€‚"

---

## Chunk 6: Remaining Specialized Parameters
**Status: [ ] Pending**

1. **`--path PATH`**
   - Current: "Server path"
   - README: "path to serve static files from (default: )<br/>(env: LLAMA_ARG_STATIC_PATH)"
   - Target: "Path to serve static files from (default: ) (env: LLAMA_ARG_STATIC_PATH) | æœåŠ¡å™¨è·¯å¾„ã€‚æä¾›é™æ€æ–‡ä»¶çš„è·¯å¾„ï¼Œç”¨äº Web UI å’Œå…¶ä»–é™æ€èµ„æºã€‚ç©ºå€¼è¡¨ç¤ºä¸æä¾›é™æ€æ–‡ä»¶æœåŠ¡ã€‚æ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ã€‚"

2. **`--api-prefix PREFIX`**
   - Current: "API prefix"
   - README: "prefix path the server serves from, without the trailing slash (default: )<br/>(env: LLAMA_ARG_API_PREFIX)"
   - Target: "Prefix path the server serves from, without the trailing slash (default: ) (env: LLAMA_ARG_API_PREFIX) | API å‰ç¼€ã€‚æœåŠ¡å™¨æä¾› API çš„å‰ç¼€è·¯å¾„ï¼Œä¸åŒ…å«å°¾éƒ¨æ–œæ ã€‚ç”¨äºè·¯å¾„ç»„ç»‡å’Œåå‘ä»£ç†é…ç½®ï¼Œæ”¯æŒå¤šä¸ªæœåŠ¡åœ¨åŒä¸€åŸŸåä¸‹è¿è¡Œã€‚"

3. **`--no-webui`**
   - Current: "Disable web UI"
   - README: "Disable the Web UI (default: enabled)<br/>(env: LLAMA_ARG_NO_WEBUI)"
   - Target: "Disable the Web UI (default: enabled) (env: LLAMA_ARG_NO_WEBUI) | ç¦ç”¨ Web UIã€‚ç¦ç”¨å†…ç½®çš„ Web ç”¨æˆ·ç•Œé¢ï¼Œä»…æä¾› API æœåŠ¡ã€‚å‡å°‘èµ„æºå ç”¨å’Œæ”»å‡»é¢ï¼Œé€‚ç”¨äºçº¯ API ä½¿ç”¨åœºæ™¯æˆ–ä¸å…¶ä»–å‰ç«¯é›†æˆçš„éƒ¨ç½²ã€‚"

4. **`--embedding, --embeddings`**
   - Current: "Enable embeddings"
   - README: "restrict to only support embedding use case; use only with dedicated embedding models (default: disabled)<br/>(env: LLAMA_ARG_EMBEDDINGS)"
   - Target: "Restrict to only support embedding use case; use only with dedicated embedding models (default: disabled) (env: LLAMA_ARG_EMBEDDINGS) | å¯ç”¨åµŒå…¥ã€‚ä»…æ”¯æŒåµŒå…¥ä½¿ç”¨åœºæ™¯ï¼Œä»…é€‚ç”¨äºä¸“ç”¨åµŒå…¥æ¨¡å‹ã€‚æä¾›æ–‡æœ¬åµŒå…¥å‘é‡ç”ŸæˆåŠŸèƒ½ï¼Œç”¨äºç›¸ä¼¼æ€§æœç´¢ã€èšç±»å’Œè¯­ä¹‰ç†è§£ç­‰ä»»åŠ¡ã€‚"

5. **`--reranking, --rerank`**
   - Current: "Enable reranking"
   - README: "enable reranking endpoint on server (default: disabled)<br/>(env: LLAMA_ARG_RERANKING)"
   - Target: "Enable reranking endpoint on server (default: disabled) (env: LLAMA_ARG_RERANKING) | å¯ç”¨é‡æ’åºã€‚åœ¨æœåŠ¡å™¨ä¸Šå¯ç”¨é‡æ’åºç«¯ç‚¹ï¼Œç”¨äºæ”¹è¿›æœç´¢ç»“æœæ’åºã€‚æä¾›æ–‡æ¡£é‡æ’åºåŠŸèƒ½ï¼Œæé«˜æ£€ç´¢ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚"

6. **`--api-key KEY`**
   - Current: "API key"
   - README: "API key to use for authentication (default: none)<br/>(env: LLAMA_API_KEY)"
   - Target: "API key to use for authentication (default: none) (env: LLAMA_API_KEY) | API å¯†é’¥ã€‚ç”¨äºèº«ä»½éªŒè¯çš„ API å¯†é’¥ï¼Œé»˜è®¤ä¸ºæ— ã€‚ä¿æŠ¤æœåŠ¡å™¨å…å—æœªæˆæƒè®¿é—®çš„é‡è¦å®‰å…¨æªæ–½ã€‚å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®¾ç½®å¼ºå¯†é’¥ã€‚"

7. **`--api-key-file FNAME`**
   - Current: "API key file"
   - README: "path to file containing API keys (default: none)"
   - Target: "Path to file containing API keys (default: none) | API å¯†é’¥æ–‡ä»¶ã€‚åŒ…å« API å¯†é’¥çš„æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒå¤šä¸ªå¯†é’¥ç®¡ç†ã€‚æä¾›æ›´å®‰å…¨çš„å¯†é’¥å­˜å‚¨æ–¹å¼ï¼Œé¿å…åœ¨å‘½ä»¤è¡Œä¸­æš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚"

8. **`--ssl-key-file FNAME`**
   - Current: "SSL key file"
   - README: "path to file a PEM-encoded SSL private key<br/>(env: LLAMA_ARG_SSL_KEY_FILE)"
   - Target: "Path to file a PEM-encoded SSL private key (env: LLAMA_ARG_SSL_KEY_FILE) | SSL å¯†é’¥æ–‡ä»¶ã€‚PEM ç¼–ç çš„ SSL ç§é’¥æ–‡ä»¶è·¯å¾„ã€‚å¯ç”¨ HTTPS åŠ å¯†é€šä¿¡ï¼Œä¿æŠ¤æ•°æ®ä¼ è¾“å®‰å…¨ã€‚éœ€è¦ä¸ SSL è¯ä¹¦æ–‡ä»¶é…åˆä½¿ç”¨ã€‚"

9. **`--ssl-cert-file FNAME`**
   - Current: "SSL certificate file"
   - README: "path to file a PEM-encoded SSL certificate<br/>(env: LLAMA_ARG_SSL_CERT_FILE)"
   - Target: "Path to file a PEM-encoded SSL certificate (env: LLAMA_ARG_SSL_CERT_FILE) | SSL è¯ä¹¦æ–‡ä»¶ã€‚PEM ç¼–ç çš„ SSL è¯ä¹¦æ–‡ä»¶è·¯å¾„ã€‚å¯ç”¨ HTTPS å®‰å…¨è¿æ¥ï¼ŒéªŒè¯æœåŠ¡å™¨èº«ä»½ã€‚å»ºè®®ä½¿ç”¨å—ä¿¡ä»» CA ç­¾å‘çš„è¯ä¹¦ã€‚"

10. **`--chat-template-kwargs STRING`**
    - Current: "Set additional params for the json template parser"
    - README: "sets additional params for the json template parser<br/>(env: LLAMA_CHAT_TEMPLATE_KWARGS)"
    - Target: "Sets additional params for the json template parser (env: LLAMA_CHAT_TEMPLATE_KWARGS) | ä¸º json æ¨¡æ¿è§£æå™¨è®¾ç½®é¢å¤–å‚æ•°ã€‚ä¸º Jinja æ¨¡æ¿è§£æå™¨æä¾›é¢å¤–çš„é…ç½®å‚æ•°ï¼Œç”¨äºè‡ªå®šä¹‰æ¨¡æ¿è¡Œä¸ºå’Œæ¸²æŸ“é€‰é¡¹ã€‚å…è®¸è°ƒæ•´æ¨¡æ¿çš„è§£ææ–¹å¼å’Œå˜é‡å¤„ç†ã€‚"

11. **`-to, --timeout N`**
    - Current: "Timeout"
    - README: "server read/write timeout in seconds (default: 600)<br/>(env: LLAMA_ARG_TIMEOUT)"
    - Target: "Server read/write timeout in seconds (default: 600) (env: LLAMA_ARG_TIMEOUT) | è¶…æ—¶æ—¶é—´ã€‚æœåŠ¡å™¨è¯»/å†™è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º 600 ç§’ï¼ˆ10åˆ†é’Ÿï¼‰ã€‚æ§åˆ¶è¯·æ±‚å’Œå“åº”çš„æœ€å¤§ç­‰å¾…æ—¶é—´ï¼Œé˜²æ­¢é•¿æ—¶é—´æŒ‚èµ·çš„è¿æ¥ã€‚"

12. **`--threads-http N`**
    - Current: "HTTP threads"
    - README: "number of threads used to process HTTP requests (default: -1)<br/>(env: LLAMA_ARG_THREADS_HTTP)"
    - Target: "Number of threads used to process HTTP requests (default: -1) (env: LLAMA_ARG_THREADS_HTTP) | HTTP çº¿ç¨‹ã€‚ç”¨äºå¤„ç† HTTP è¯·æ±‚çš„çº¿ç¨‹æ•°ï¼Œ-1 è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ã€‚å½±å“æœåŠ¡å™¨çš„å¹¶å‘å¤„ç†èƒ½åŠ›ï¼Œè¾ƒé«˜çš„å€¼æ”¯æŒæ›´å¤šåŒæ—¶è¿æ¥ä½†å¢åŠ èµ„æºä½¿ç”¨ã€‚"

13. **`--cache-reuse N`**
    - Current: "Min chunk size to attempt reusing from the cache via KV shifting"
    - README: "min chunk size to attempt reusing from the cache via KV shifting (default: 0)<br/>[(card)](https://ggml.ai/f0.png)<br/>(env: LLAMA_ARG_CACHE_REUSE)"
    - Target: "Min chunk size to attempt reusing from the cache via KV shifting (default: 0) (env: LLAMA_ARG_CACHE_REUSE) | ç¼“å­˜é‡ç”¨ã€‚é€šè¿‡ KV ç§»ä½å°è¯•é‡ç”¨ç¼“å­˜çš„æœ€å°å—å¤§å°ã€‚ç¼“å­˜é‡ç”¨æœºåˆ¶å¯ä»¥é€šè¿‡ç§»åŠ¨ KV ç¼“å­˜æ¥é‡ç”¨ä¹‹å‰è®¡ç®—çš„çŠ¶æ€ï¼Œæé«˜è¿ç»­æ¨ç†çš„æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›¸ä¼¼çš„æç¤ºè¯åºåˆ—ä¸­ã€‚"

14. **`--metrics`**
    - Current: "Enable metrics"
    - README: "enable prometheus compatible metrics endpoint (default: disabled)<br/>(env: LLAMA_ARG_ENDPOINT_METRICS)"
    - Target: "Enable prometheus compatible metrics endpoint (default: disabled) (env: LLAMA_ARG_ENDPOINT_METRICS) | å¯ç”¨æŒ‡æ ‡ã€‚å¯ç”¨ä¸ Prometheus å…¼å®¹çš„æŒ‡æ ‡ç«¯ç‚¹ï¼Œæä¾›æ€§èƒ½ç›‘æ§æ•°æ®ã€‚ä¾¿äºé›†æˆåˆ°ç›‘æ§ç³»ç»Ÿä¸­ï¼Œå®ç°å®æ—¶æ€§èƒ½è·Ÿè¸ªå’Œå‘Šè­¦ã€‚"

15. **`--props`**
    - Current: "Enable properties"
    - README: "enable changing global properties via POST /props (default: disabled)<br/>(env: LLAMA_ARG_ENDPOINT_PROPS)"
    - Target: "Enable changing global properties via POST /props (default: disabled) (env: LLAMA_ARG_ENDPOINT_PROPS) | å¯ç”¨å±æ€§ã€‚å¯ç”¨é€šè¿‡ POST /props æ›´æ”¹å…¨å±€å±æ€§çš„ç«¯ç‚¹ã€‚å…è®¸è¿è¡Œæ—¶é…ç½®è°ƒæ•´ï¼Œæä¾›åŠ¨æ€å‚æ•°ç®¡ç†èƒ½åŠ›ã€‚"

16. **`--slots`**
    - Current: "Enable slots"
    - README: "enable slots monitoring endpoint (default: enabled)<br/>(env: LLAMA_ARG_ENDPOINT_SLOTS)"
    - Target: "Enable slots monitoring endpoint (default: enabled) (env: LLAMA_ARG_ENDPOINT_SLOTS) | å¯ç”¨æ§½ä½ã€‚å¯ç”¨æ§½ä½ç›‘æ§ç«¯ç‚¹ï¼Œæä¾›å¤„ç†æ§½ä½çš„å®æ—¶çŠ¶æ€ä¿¡æ¯ã€‚é»˜è®¤å¯ç”¨ï¼Œå¯¹äºç›‘æ§å’Œè°ƒè¯•å¤šç”¨æˆ·å¹¶å‘å¤„ç†éå¸¸é‡è¦ã€‚"

17. **`--no-slots`**
    - Current: "Disable slots"
    - README: "disables slots monitoring endpoint<br/>(env: LLAMA_ARG_NO_ENDPOINT_SLOTS)"
    - Target: "Disables slots monitoring endpoint (env: LLAMA_ARG_NO_ENDPOINT_SLOTS) | ç¦ç”¨æ§½ä½ã€‚ç¦ç”¨æ§½ä½ç›‘æ§ç«¯ç‚¹ï¼Œå‡å°‘ç›‘æ§å¼€é”€ä½†å¤±å»å¤„ç†çŠ¶æ€å¯è§æ€§ã€‚åœ¨ä¸éœ€è¦ç›‘æ§çš„ç®€å•éƒ¨ç½²ä¸­å¯è€ƒè™‘ä½¿ç”¨ã€‚"

18. **`--slot-save-path PATH`**
    - Current: "Slot save path"
    - README: "path to save slot kv cache (default: disabled)"
    - Target: "Path to save slot kv cache (default: disabled) | æ§½ä½ä¿å­˜è·¯å¾„ã€‚ä¿å­˜æ§½ä½ KV ç¼“å­˜çš„è·¯å¾„ï¼Œé»˜è®¤ç¦ç”¨ã€‚æ”¯æŒæŒä¹…åŒ–å¤„ç†çŠ¶æ€ï¼Œå®ç°ä¼šè¯æ¢å¤å’ŒçŠ¶æ€ä¿æŒåŠŸèƒ½ã€‚"

19. **`--jinja`**
    - Current: "Use Jinja templating"
    - README: "use jinja template for chat (default: disabled)<br/>(env: LLAMA_ARG_JINJA)"
    - Target: "Use jinja template for chat (default: disabled) (env: LLAMA_ARG_JINJA) | å¯ç”¨ Jinja æ¨¡æ¿ã€‚ä½¿ç”¨ Jinja æ¨¡æ¿è¿›è¡ŒèŠå¤©ï¼Œæä¾›æ›´çµæ´»çš„æç¤ºæ¨¡æ¿æ”¯æŒã€‚å…è®¸è‡ªå®šä¹‰å¤æ‚çš„æç¤ºæ„å»ºé€»è¾‘ï¼Œæ»¡è¶³ç‰¹å®šå¯¹è¯éœ€æ±‚ã€‚"

20. **`--reasoning-format FORMAT`**
    - Current: "Reasoning format"
    - README: "controls whether thought tags are allowed and/or extracted from the response, and in which format they're returned; one of:<br/>- none: leaves thoughts unparsed in `message.content`<br/>- deepseek: puts thoughts in `message.reasoning_content` (except in streaming mode, which behaves as `none`)<br/>(default: auto)<br/>(env: LLAMA_ARG_THINK)"
    - Target: "Controls whether thought tags are allowed and/or extracted from the response, and in which format they're returned; one of: none, deepseek (default: auto) (env: LLAMA_ARG_THINK) | æ¨ç†æ ¼å¼ã€‚æ§åˆ¶æ˜¯å¦å…è®¸å’Œ/æˆ–ä»å“åº”ä¸­æå–æ€è€ƒæ ‡ç­¾ï¼Œä»¥åŠè¿”å›æ ¼å¼ï¼šnoneï¼ˆæ€æƒ³ä¿ç•™åœ¨ message.content ä¸­ï¼‰ã€deepseekï¼ˆæ€æƒ³æ”¾åœ¨ message.reasoning_content ä¸­ï¼‰ã€‚é»˜è®¤ä¸º autoã€‚"

---

## Additional Parameters to Complete

21. **`--reasoning-budget N`**
    - Current: "Reasoning budget"
    - README: "controls the amount of thinking allowed; currently only one of: -1 for unrestricted thinking budget, or 0 to disable thinking (default: -1)<br/>(env: LLAMA_ARG_THINK_BUDGET)"
    - Target: "Controls the amount of thinking allowed; currently only one of: -1 for unrestricted thinking budget, or 0 to disable thinking (default: -1) (env: LLAMA_ARG_THINK_BUDGET) | æ¨ç†é¢„ç®—ã€‚æ§åˆ¶å…è®¸çš„æ€è€ƒé‡ï¼Œç›®å‰åªæœ‰ -1 è¡¨ç¤ºæ— é™åˆ¶æ€è€ƒé¢„ç®—ï¼Œæˆ– 0 è¡¨ç¤ºç¦ç”¨æ€è€ƒã€‚é»˜è®¤ä¸º -1ã€‚å¹³è¡¡æ€è€ƒæ·±åº¦å’Œå“åº”é€Ÿåº¦ã€‚"

22. **`--chat-template JINJA_TEMPLATE`**
    - Current: "Set custom jinja chat template"
    - README: "set custom jinja chat template (default: template taken from model's metadata)<br/>if suffix/prefix are specified, template will be disabled<br/>only commonly used templates are accepted (unless --jinja is set before this flag):<br/>list of built-in templates:<br/>bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge, gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world, seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr<br/>(env: LLAMA_ARG_CHAT_TEMPLATE)"
    - Target: "Set custom jinja chat template (env: LLAMA_ARG_CHAT_TEMPLATE) | è®¾ç½®è‡ªå®šä¹‰ jinja èŠå¤©æ¨¡æ¿ã€‚æŒ‡å®šè‡ªå®šä¹‰çš„ Jinja èŠå¤©æ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–å¯¹è¯è¾“å…¥ã€‚å†…ç½®æ¨¡æ¿åˆ—è¡¨ï¼šbailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge, gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world, seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr"

23. **`--chat-template-file JINJA_TEMPLATE_FILE`**
    - Current: "Set custom jinja chat template file"
    - README: "set custom jinja chat template file (default: template taken from model's metadata)<br/>if suffix/prefix are specified, template will be disabled<br/>only commonly used templates are accepted (unless --jinja is set before this flag):<br/>list of built-in templates:<br/>bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge, gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world, seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr<br/>(env: LLAMA_ARG_CHAT_TEMPLATE_FILE)"
    - Target: "Set custom jinja chat template file (env: LLAMA_ARG_CHAT_TEMPLATE_FILE) | è®¾ç½®è‡ªå®šä¹‰ jinja èŠå¤©æ¨¡æ¿æ–‡ä»¶ã€‚ä»æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰çš„ Jinja èŠå¤©æ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–å¯¹è¯è¾“å…¥ã€‚å†…ç½®æ¨¡æ¿åˆ—è¡¨ä¸ --chat-template ç›¸åŒã€‚"

24. **`--no-prefill-assistant`**
    - Current: "Whether to prefill the assistant's response if the last message is an assistant message"
    - README: "whether to prefill the assistant's response if the last message is an assistant message (default: prefill enabled)<br/>when this flag is set, if the last message is an assistant message then it will be treated as a full message and not prefilled<br/><br/>(env: LLAMA_ARG_NO_PREFILL_ASSISTANT)"
    - Target: "Whether to prefill the assistant's response if the last message is an assistant message (default: prefill enabled) (env: LLAMA_ARG_NO_PREFILL_ASSISTANT) | æ˜¯å¦é¢„å¡«å……åŠ©æ‰‹å“åº”ã€‚æ§åˆ¶å½“æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯åŠ©æ‰‹æ¶ˆæ¯æ—¶æ˜¯å¦é¢„å¡«å……åŠ©æ‰‹çš„å›å¤ã€‚é»˜è®¤å¯ç”¨é¢„å¡«å……ã€‚è®¾ç½®æ­¤æ ‡å¿—åï¼Œå¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ˜¯åŠ©æ‰‹æ¶ˆæ¯ï¼Œåˆ™å°†å…¶è§†ä¸ºå®Œæ•´æ¶ˆæ¯è€Œä¸è¿›è¡Œé¢„å¡«å……ã€‚"

25. **`-sps, --slot-prompt-similarity SIMILARITY`**
    - Current: "How much the prompt of a request must match the prompt of a slot in order to use that slot"
    - README: "how much the prompt of a request must match the prompt of a slot in order to use that slot (default: 0.50, 0.0 = disabled)<br/>"
    - Target: "How much the prompt of a request must match the prompt of a slot in order to use that slot (default: 0.50, 0.0 = disabled) (env: LLAMA_ARG_SLOT_PROMPT_SIMILARITY) | æ§½ä½æç¤ºç›¸ä¼¼åº¦ã€‚è¯·æ±‚çš„æç¤ºè¯å¿…é¡»ä¸ slot çš„æç¤ºè¯åŒ¹é…çš„ç¨‹åº¦æ‰èƒ½ä½¿ç”¨è¯¥ slotã€‚è¾ƒé«˜çš„å€¼æé«˜é‡ç”¨çš„å‡†ç¡®æ€§ä½†å¯èƒ½é™ä½å‘½ä¸­ç‡ï¼Œè¾ƒä½çš„å€¼å¢åŠ é‡ç”¨æœºä¼šä½†å¯èƒ½å½±å“è´¨é‡ã€‚"

26. **`--lora-init-without-apply`**
    - Current: "Initialize LoRA without applying"
    - README: "load LoRA adapters without applying them (apply later via POST /lora-adapters) (default: disabled)"
    - Target: "Load LoRA adapters without applying them (apply later via POST /lora-adapters) (default: disabled) (env: LLAMA_ARG_LORA_INIT_WITHOUT_APPLY) | åˆå§‹åŒ– LoRA ä½†ä¸åº”ç”¨ã€‚åŠ è½½ LoRA é€‚é…å™¨ä½†ä¸ç«‹å³åº”ç”¨ï¼ˆç¨åé€šè¿‡ POST /lora-adapters åº”ç”¨ï¼‰ã€‚æ”¯æŒåŠ¨æ€ LoRA ç®¡ç†ï¼Œå¯åœ¨è¿è¡Œæ—¶åˆ‡æ¢ä¸åŒçš„é€‚é…å™¨é…ç½®ã€‚"

27. **`--draft-max, --draft, --draft-n N`**
    - Current: "Number of tokens to draft for speculative decoding"
    - README: "number of tokens to draft for speculative decoding (default: 16)<br/>(env: LLAMA_ARG_DRAFT_MAX)"
    - Target: "Number of tokens to draft for speculative decoding (default: 16) (env: LLAMA_ARG_DRAFT_MAX) | æŠ•æœºè§£ç çš„è‰ç¨¿ token æ•°é‡ã€‚è‰ç¨¿æ¨¡å‹å•æ¬¡ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼Œå½±å“æŠ•æœºè§£ç çš„æ•ˆç‡ã€‚è¾ƒå¤§çš„å€¼å¯èƒ½æé«˜é€Ÿåº¦ä½†å¢åŠ è‰ç¨¿è¢«æ‹’ç»çš„é£é™©ï¼Œéœ€è¦æ ¹æ®æ¨¡å‹è´¨é‡å’Œåº”ç”¨åœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚"

28. **`--draft-min, --draft-n-min N`**
    - Current: "Minimum number of draft tokens to use for speculative decoding"
    - README: "minimum number of draft tokens to use for speculative decoding (default: 0)<br/>(env: LLAMA_ARG_DRAFT_MIN)"
    - Target: "Minimum number of draft tokens to use for speculative decoding (default: 0) (env: LLAMA_ARG_DRAFT_MIN) | æŠ•æœºè§£ç ä½¿ç”¨çš„æœ€å°è‰ç¨¿ token æ•°é‡ã€‚å½“è‰ç¨¿ç”Ÿæˆè´¨é‡è¾ƒä½æ—¶ï¼Œå®é™…ç”Ÿæˆçš„ token æ•°é‡å¯èƒ½ä½äºæ­¤å€¼ã€‚è®¾ç½®ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶æœ€å°å€¼ï¼Œå…è®¸çµæ´»çš„æŠ•æœºç­–ç•¥ã€‚"

29. **`--draft-p-min P`**
    - Current: "Minimum speculative decoding probability"
    - README: "minimum speculative decoding probability (greedy) (default: 0.8)<br/>(env: LLAMA_ARG_DRAFT_P_MIN)"
    - Target: "Minimum speculative decoding probability (greedy) (default: 0.8) (env: LLAMA_ARG_DRAFT_P_MIN) | æœ€å°æŠ•æœºè§£ç æ¦‚ç‡ã€‚è‰ç¨¿ token è¢«æ¥å—çš„æœ€å°æ¦‚ç‡é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼çš„ token å°†è¢«æ¥å—ã€‚è¾ƒé«˜çš„é˜ˆå€¼æé«˜è¾“å‡ºè´¨é‡ä½†å¯èƒ½å‡å°‘é€Ÿåº¦æå‡ï¼Œè¾ƒä½çš„é˜ˆå€¼å¢åŠ é€Ÿåº¦ä½†å¯èƒ½é™ä½è´¨é‡ã€‚"

30. **`-cd, --ctx-size-draft N`**
    - Current: "Size of the prompt context for the draft model"
    - README: "size of the prompt context for the draft model (default: 0, 0 = loaded from model)<br/>(env: LLAMA_ARG_CTX_SIZE_DRAFT)"
    - Target: "Size of the prompt context for the draft model (default: 0, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE_DRAFT) | è‰ç¨¿æ¨¡å‹çš„æç¤ºè¯ä¸Šä¸‹æ–‡å¤§å°ã€‚è‰ç¨¿æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œå¯ä»¥ä¸ä¸»æ¨¡å‹ä¸åŒã€‚è®¾ç½®ä¸º 0 å°†ä»æ¨¡å‹å…ƒæ•°æ®ä¸­åŠ è½½é»˜è®¤å€¼ã€‚è¾ƒå¤§çš„ä¸Šä¸‹æ–‡å¯èƒ½æé«˜è‰ç¨¿è´¨é‡ä½†å¢åŠ å†…å­˜ä½¿ç”¨ã€‚"

31. **`-devd, --device-draft <dev1,dev2,..>`**
    - Current: "Comma-separated list of devices to use for offloading the draft model"
    - README: "comma-separated list of devices to use for offloading the draft model (none = don't offload). use --list-devices to see a list of available devices"
    - Target: "Comma-separated list of devices to use for offloading the draft model (none = don't offload). Use --list-devices to see a list of available devices (env: LLAMA_ARG_DEVICE_DRAFT) | è‰ç¨¿æ¨¡å‹è®¾å¤‡ã€‚ç”¨äºå¸è½½è‰ç¨¿æ¨¡å‹çš„é€—å·åˆ†éš”è®¾å¤‡åˆ—è¡¨ï¼ˆnone = ä¸å¸è½½ï¼‰ã€‚è‰ç¨¿æ¨¡å‹å¯ä»¥ä¸ä¸»æ¨¡å‹ä½¿ç”¨ä¸åŒçš„è®¾å¤‡é…ç½®ï¼Œä»¥ä¼˜åŒ–èµ„æºåˆ©ç”¨å’Œæ€§èƒ½ã€‚"

32. **`-ngld, --gpu-layers-draft, --n-gpu-layers-draft N`**
    - Current: "Number of layers to store in VRAM for the draft model"
    - README: "number of layers to store in VRAM for the draft model<br/>(env: LLAMA_ARG_N_GPU_LAYERS_DRAFT)"
    - Target: "Number of layers to store in VRAM for the draft model (env: LLAMA_ARG_N_GPU_LAYERS_DRAFT) | è‰ç¨¿æ¨¡å‹ GPU å±‚æ•°ã€‚å­˜å‚¨åœ¨è‰ç¨¿æ¨¡å‹ VRAM ä¸­çš„å±‚æ•°ã€‚è‰ç¨¿æ¨¡å‹å¸è½½åˆ° GPU çš„ç¥ç»ç½‘ç»œå±‚æ•°ï¼Œå¯ä»¥ä¸ä¸»æ¨¡å‹çš„ GPU å±‚æ•°è®¾ç½®ä¸åŒã€‚åˆç†çš„è®¾ç½®å¯ä»¥åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æœ€å¤§åŒ–èµ„æºåˆ©ç”¨æ•ˆç‡ã€‚"

33. **`-md, --model-draft FNAME`**
    - Current: "Draft model for speculative decoding"
    - README: "draft model for speculative decoding (default: unused)<br/>(env: LLAMA_ARG_MODEL_DRAFT)"
    - Target: "Draft model for speculative decoding (default: unused) (env: LLAMA_ARG_MODEL_DRAFT) | è‰ç¨¿æ¨¡å‹ã€‚ç”¨äºæŠ•æœºè§£ç çš„è‰ç¨¿æ¨¡å‹ï¼ˆé»˜è®¤ï¼šæœªä½¿ç”¨ï¼‰ã€‚æŠ•æœºè§£ç ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„è‰ç¨¿æ¨¡å‹å¿«é€Ÿç”Ÿæˆå€™é€‰ tokenï¼Œç„¶åç”±ä¸»æ¨¡å‹éªŒè¯ï¼Œå¯æ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦ã€‚è‰ç¨¿æ¨¡å‹åº”è¯¥æ¯”ä¸»æ¨¡å‹å°ä½†è´¨é‡ç›¸è¿‘ï¼Œä»¥ç¡®ä¿è¾ƒé«˜çš„æ¥å—ç‡ã€‚"

34. **`--spec-replace TARGET DRAFT`**
    - Current: "Translate the string in TARGET into DRAFT if the draft model and main model are not compatible"
    - README: "translate the string in TARGET into DRAFT if the draft model and main model are not compatible"
    - Target: "Translate the string in TARGET into DRAFT if the draft model and main model are not compatible (env: LLAMA_ARG_SPEC_REPLACE) | è§„æ ¼æ›¿æ¢ã€‚å¦‚æœè‰ç¨¿æ¨¡å‹å’Œä¸»æ¨¡å‹ä¸å…¼å®¹ï¼Œå°† TARGET ä¸­çš„å­—ç¬¦ä¸²ç¿»è¯‘ä¸º DRAFTã€‚å½“è‰ç¨¿æ¨¡å‹å’Œä¸»æ¨¡å‹ä½¿ç”¨ä¸åŒçš„ tokenizer æˆ–è¯æ±‡è¡¨æ—¶ï¼Œæ­¤å‚æ•°å¯ç”¨äºå¤„ç† token æ˜ å°„é—®é¢˜ï¼Œç¡®ä¿æŠ•æœºè§£ç çš„æ­£ç¡®å·¥ä½œã€‚"

35. **`-mv, --model-vocoder FNAME`**
    - Current: "Vocoder model for audio generation"
    - README: "vocoder model for audio generation (default: unused)"
    - Target: "Vocoder model for audio generation (default: unused) (env: LLAMA_ARG_MODEL_VOCODER) | å£°ç å™¨æ¨¡å‹ã€‚ç”¨äºéŸ³é¢‘ç”Ÿæˆçš„å£°ç å™¨æ¨¡å‹ï¼ˆé»˜è®¤ï¼šæœªä½¿ç”¨ï¼‰ã€‚å£°ç å™¨æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ï¼Œå®ç°æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åŠŸèƒ½ã€‚éœ€è¦ä¸æ”¯æŒçš„è¯­éŸ³åˆæˆæ¨¡å‹é…åˆä½¿ç”¨ï¼Œå¯ç”Ÿæˆè‡ªç„¶æµç•…çš„è¯­éŸ³è¾“å‡ºã€‚"

36. **`--tts-use-guide-tokens`**
    - Current: "Use guide tokens to improve TTS word recall"
    - README: "Use guide tokens to improve TTS word recall"
    - Target: "Use guide tokens to improve TTS word recall (env: LLAMA_ARG_TTS_USE_GUIDE_TOKENS) | ä½¿ç”¨å¼•å¯¼æ ‡è®°æ”¹å–„ TTS è¯å¬å›ç‡ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­ä½¿ç”¨å¼•å¯¼æ ‡è®°æ¥æé«˜å•è¯è¯†åˆ«å‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨æ–‡æœ¬ä¸­æ·»åŠ ç‰¹æ®Šçš„å¼•å¯¼æ ‡è®°ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å•è¯è¾¹ç•Œå’Œå‘éŸ³ï¼Œä»è€Œæ”¹å–„è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚"

37. **`--embd-bge-small-en-default`**
    - Current: "Use default bge-small-en-v1.5 model"
    - README: "use default bge-small-en-v1.5 model (note: can download weights from the internet)"
    - Target: "Use default bge-small-en-v1.5 model (note: can download weights from the internet) (env: LLAMA_ARG_EMBD_BGE_SMALL_EN_DEFAULT) | ä½¿ç”¨é»˜è®¤çš„ bge-small-en-v1.5 æ¨¡å‹ã€‚BGEï¼ˆBAAI General Embeddingï¼‰æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„è‹±æ–‡æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œé€‚ç”¨äºè¯­ä¹‰æœç´¢ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ç­‰ä»»åŠ¡ã€‚å¯ç”¨åå°†è‡ªåŠ¨ä¸‹è½½æ‰€éœ€æ¨¡å‹æ–‡ä»¶ã€‚"

38. **`--embd-e5-small-en-default`**
    - Current: "Use default e5-small-v2 model"
    - README: "use default e5-small-v2 model (note: can download weights from the internet)"
    - Target: "Use default e5-small-v2 model (note: can download weights from the internet) (env: LLAMA_ARG_EMBD_E5_SMALL_EN_DEFAULT) | ä½¿ç”¨é»˜è®¤çš„ e5-small-v2 æ¨¡å‹ã€‚E5 æ˜¯ä¸€ä¸ªåŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œåœ¨å„ç§è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç‰¹åˆ«é€‚åˆçŸ­æ–‡æœ¬åŒ¹é…å’Œè¯­ä¹‰æ£€ç´¢åº”ç”¨ã€‚"

39. **`--embd-gte-small-default`**
    - Current: "Use default gte-small model"
    - README: "use default gte-small model (note: can download weights from the internet)"
    - Target: "Use default gte-small model (note: can download weights from the internet) (env: LLAMA_ARG_EMBD_GTE_SMALL_DEFAULT) | ä½¿ç”¨é»˜è®¤çš„ gte-small æ¨¡å‹ã€‚GTEï¼ˆGeneral Text Embeddingï¼‰æ˜¯ä¸€ä¸ªé€šç”¨çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œåœ¨å¤šç§è¯­è¨€å’Œä»»åŠ¡ä¸Šéƒ½å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«é€‚åˆå¤šè¯­è¨€åº”ç”¨åœºæ™¯ã€‚"

40. **`fim-qwen1.5b-default`**
    - Current: "Use default Qwen 2.5 Coder 1.5B"
    - README: "use default Qwen 2.5 Coder 1.5B (note: can download weights from the internet)"
    - Target: "Use default Qwen 2.5 Coder 1.5B (note: can download weights from the internet) (env: LLAMA_ARG_FIM_QWEN1_5B_DEFAULT) | ä½¿ç”¨é»˜è®¤çš„ Qwen 2.5 Coder 1.5Bã€‚è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºä»£ç å¡«å……ï¼ˆFill-in-the-Middleï¼‰çš„ 1.5B å‚æ•°æ¨¡å‹ï¼Œé€‚ç”¨äºä»£ç è¡¥å…¨ã€ä»£ç ä¿®å¤ç­‰ä»»åŠ¡ã€‚å¯ç”¨åå°†è‡ªåŠ¨ä¸‹è½½æ‰€éœ€æ¨¡å‹æ–‡ä»¶ã€‚"

41. **`fim-qwen3b-default`**
    - Current: "Use default Qwen 2.5 Coder 3B"
    - README: "use default Qwen 2.5 Coder 3B (note: can download weights from the internet)"
    - Target: "Use default Qwen 2.5 Coder 3B (note: can download weights from the internet) (env: LLAMA_ARG_FIM_QWEN3B_DEFAULT) | ä½¿ç”¨é»˜è®¤çš„ Qwen 2.5 Coder 3Bã€‚è¿™æ˜¯ä¸€ä¸ª 3B å‚æ•°çš„ä»£ç å¡«å……æ¨¡å‹ï¼Œåœ¨ä»£ç è¡¥å…¨è´¨é‡ä¸Šä¼˜äº 1.5B ç‰ˆæœ¬ï¼Œé€‚ç”¨äºæ›´å¤æ‚çš„ä»£ç ç”Ÿæˆå’Œä¿®å¤ä»»åŠ¡ã€‚å¹³è¡¡äº†æ€§èƒ½å’Œèµ„æºä½¿ç”¨ã€‚"

42. **`fim-qwen7b-default`**
    - Current: "Use default Qwen 2.5 Coder 7B"
    - README: "use default Qwen 2.5 Coder 7B (note: can download weights from the internet)"
    - Target: "Use default Qwen 2.5 Coder 7B (note: can download weights from the internet) (env: LLAMA_ARG_FIM_QWEN7B_DEFAULT) | ä½¿ç”¨é»˜è®¤çš„ Qwen 2.5 Coder 7Bã€‚è¿™æ˜¯ä¸€ä¸ª 7B å‚æ•°çš„é«˜è´¨é‡ä»£ç å¡«å……æ¨¡å‹ï¼Œåœ¨ä»£ç ç†è§£ã€ç”Ÿæˆå’Œä¿®å¤æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚é€‚ç”¨äºå¯¹ä»£ç è´¨é‡è¦æ±‚è¾ƒé«˜çš„ä¸“ä¸šåœºæ™¯ã€‚"

43. **`fim-qwen7b-spec`**
    - Current: "Use Qwen 2.5 Coder 7B + 0.5B draft for speculative decoding"
    - README: "use Qwen 2.5 Coder 7B + 0.5B draft for speculative decoding (note: can download weights from the internet)"
    - Target: "Use Qwen 2.5 Coder 7B + 0.5B draft for speculative decoding (note: can download weights from the internet) (env: LLAMA_ARG_FIM_QWEN7B_SPEC) | ä½¿ç”¨ Qwen 2.5 Coder 7B + 0.5B è‰ç¨¿æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„ä»£ç å¡«å……é…ç½®ï¼Œä½¿ç”¨ 7B ä¸»æ¨¡å‹é…åˆ 0.5B è‰ç¨¿æ¨¡å‹ï¼Œé€šè¿‡æŠ•æœºè§£ç æŠ€æœ¯æ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚"

44. **`fim-qwen14b-spec`**
    - Current: "Use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding"
    - README: "use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding (note: can download weights from the internet)"
    - Target: "Use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding (note: can download weights from the internet) (env: LLAMA_ARG_FIM_QWEN14B_SPEC) | ä½¿ç”¨ Qwen 2.5 Coder 14B + 0.5B è‰ç¨¿æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ä»£ç å¡«å……é…ç½®ï¼Œä½¿ç”¨ 14B ä¸»æ¨¡å‹é…åˆ 0.5B è‰ç¨¿æ¨¡å‹ï¼Œæä¾›é¡¶çº§çš„ä»£ç ç†è§£ã€ç”Ÿæˆå’Œä¿®å¤èƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡æŠ•æœºè§£ç å®ç°å¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚"

45. **`fim-qwen30b-default`**
    - Current: "Use default Qwen 3 Coder 30B A3B Instruct"
    - README: "use default Qwen 3 Coder 30B A3B Instruct (note: can download weights from the internet)"
    - Target: "Use default Qwen 3 Coder 30B A3B Instruct (note: can download weights from the internet) (env: LLAMA_ARG_FIM_QWEN30B_DEFAULT) | ä½¿ç”¨é»˜è®¤çš„ Qwen 3 Coder 30B A3B Instructã€‚è¿™æ˜¯ä¸€ä¸ªè¶…å¤§å‹çš„ 30B å‚æ•°ä»£ç æ¨¡å‹ï¼Œå…·æœ‰æœ€å¼ºçš„ä»£ç ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œé€‚ç”¨äºæœ€å¤æ‚çš„ä»£ç ä»»åŠ¡å’Œä¸“ä¸šçš„å¼€å‘åœºæ™¯ã€‚éœ€è¦è¾ƒé«˜çš„ç¡¬ä»¶é…ç½®æ‰èƒ½æœ‰æ•ˆè¿è¡Œã€‚"

---

## Sampling Parameters (Full List)

1. **`--samplers SAMPLERS`**
   - Current: "Samplers configuration"
   - README: "samplers that will be used for generation in the order, separated by ';'<br/>(default: penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)"
   - Target: "Samplers that will be used for generation in the order, separated by ';' (default: penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature) | é‡‡æ ·å™¨é…ç½®ã€‚æŒ‡å®šç”Ÿæˆæ—¶ä½¿ç”¨çš„é‡‡æ ·å™¨åºåˆ—ï¼Œç”¨åˆ†å·åˆ†éš”ã€‚é»˜è®¤åºåˆ—ï¼špenalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperatureã€‚é‡‡æ ·å™¨çš„é¡ºåºå½±å“æœ€ç»ˆç”Ÿæˆç»“æœã€‚"

2. **`-s, --seed SEED`**
   - Current: "RNG seed"
   - README: "RNG seed (default: -1, use random seed for -1)"
   - Target: "RNG seed (default: -1, use random seed for -1) | RNG ç§å­ã€‚éšæœºæ•°ç”Ÿæˆå™¨ç§å­ï¼Œæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹çš„éšæœºæ€§ã€‚-1 è¡¨ç¤ºä½¿ç”¨éšæœºç§å­ã€‚ç›¸åŒçš„ç§å­ä¼šäº§ç”Ÿç›¸åŒçš„è¾“å‡ºï¼Œç”¨äºç»“æœé‡ç°å’Œè°ƒè¯•ã€‚"

3. **`--sampling-seq, --sampler-seq SEQUENCE`**
   - Current: "Sampling sequence"
   - README: "simplified sequence for samplers that will be used (default: edskypmxt)"
   - Target: "Simplified sequence for samplers that will be used (default: edskypmxt) | é‡‡æ ·åºåˆ—ã€‚é‡‡æ ·å™¨çš„ç®€åŒ–åºåˆ—è¡¨ç¤ºï¼Œæ¯ä¸ªå­—ç¬¦å¯¹åº”ä¸€ä¸ªé‡‡æ ·å™¨ã€‚é»˜è®¤ 'edskypmxt'ã€‚æä¾›æ›´ç®€æ´çš„æ–¹å¼æ¥é…ç½®é‡‡æ ·å™¨é¡ºåºï¼Œä¾¿äºå¿«é€Ÿè°ƒæ•´ã€‚"

4. **`--ignore-eos`**
   - Current: "Ignore end of sequence token"
   - README: "ignore end of stream token and continue generating (implies --logit-bias EOS-inf)"
   - Target: "Ignore end of stream token and continue generating (implies --logit-bias EOS-inf) | å¿½ç•¥åºåˆ—ç»“æŸ tokenã€‚å¿½ç•¥ EOSï¼ˆç»“æŸåºåˆ—ï¼‰token å¹¶ç»§ç»­ç”Ÿæˆï¼Œæ„å‘³ç€ --logit-bias EOS-infã€‚é€‚ç”¨äºéœ€è¦å¼ºåˆ¶ç”Ÿæˆæ›´é•¿çš„æ–‡æœ¬æˆ–ä¸å¸Œæœ›æ¨¡å‹æå‰ç»“æŸçš„åœºæ™¯ã€‚"

5. **`--temp N`**
   - Current: "Temperature for sampling"
   - README: "temperature (default: 0.8)"
   - Target: "Temperature (default: 0.8) | é‡‡æ ·æ¸©åº¦ã€‚æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œè¾ƒä½çš„å€¼ï¼ˆå¦‚ 0.2ï¼‰ä½¿è¾“å‡ºæ›´ç¡®å®šæ€§ï¼Œè¾ƒé«˜çš„å€¼ï¼ˆå¦‚ 1.0ï¼‰å¢åŠ éšæœºæ€§å’Œåˆ›é€ æ€§ã€‚é»˜è®¤ 0.8 æä¾›è‰¯å¥½çš„å¹³è¡¡ã€‚"

6. **`--top-k N`**
   - Current: "Top-k sampling"
   - README: "top-k sampling (default: 40, 0 = disabled)"
   - Target: "Top-k sampling (default: 40, 0 = disabled) | Top-k é‡‡æ ·ã€‚ä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­éšæœºé€‰æ‹©ï¼Œé™åˆ¶å€™é€‰è¯æ•°é‡ã€‚è¾ƒé«˜çš„ k å€¼å¢åŠ å¤šæ ·æ€§ï¼Œè¾ƒä½çš„ k å€¼ä½¿è¾“å‡ºæ›´é›†ä¸­ã€‚0 è¡¨ç¤ºç¦ç”¨ï¼Œä»…ä»æœ€å¯èƒ½çš„è¯ä¸­é€‰æ‹©ã€‚"

7. **`--top-p N`**
   - Current: "Top-p sampling"
   - README: "top-p sampling (default: 0.9, 1.0 = disabled)"
   - Target: "Top-p sampling (default: 0.9, 1.0 = disabled) | Top-p æ ¸å¿ƒé‡‡æ ·ã€‚ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„æœ€å° token é›†åˆä¸­è¿›è¡Œé‡‡æ ·ï¼Œé™åˆ¶å€™é€‰è¯æ•°é‡ã€‚0.9 è¡¨ç¤ºä»ç´¯ç§¯æ¦‚ç‡ 90% çš„è¯ä¸­é€‰æ‹©ã€‚1.0 è¡¨ç¤ºç¦ç”¨ã€‚é€‚ç”¨äºæ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§ã€‚"

8. **`--min-p N`**
   - Current: "Min-p sampling"
   - README: "min-p sampling (default: 0.1, 0.0 = disabled)"
   - Target: "Min-p sampling (default: 0.1, 0.0 = disabled) | Min-p é‡‡æ ·ã€‚æœ€å°æ¦‚ç‡é‡‡æ ·ï¼Œè¿‡æ»¤æ‰æ¦‚ç‡ä½äºæœ€é«˜æ¦‚ç‡ä¹˜ä»¥ p çš„ tokenã€‚å¯é˜²æ­¢ä½è´¨é‡ token è¢«é€‰ä¸­ï¼Œæé«˜è¾“å‡ºè´¨é‡ã€‚0.0 è¡¨ç¤ºç¦ç”¨ã€‚"

9. **`--top-nsigma N`**
   - Current: "Top-n sigma sampling"
   - README: "top-n-sigma sampling (default: -1.0, -1.0 = disabled)"
   - Target: "Top-n-sigma sampling (default: -1.0, -1.0 = disabled) | Top-n sigma é‡‡æ ·ã€‚åŸºäºæ ‡å‡†å·®çš„é‡‡æ ·æ–¹æ³•ï¼Œè€ƒè™‘æ¦‚ç‡åˆ†å¸ƒçš„æ ‡å‡†å·®ã€‚è´Ÿå€¼è¡¨ç¤ºç¦ç”¨ã€‚æä¾›å¦ä¸€ç§æ§åˆ¶è¾“å‡ºå¤šæ ·æ€§çš„æ–¹æ³•ï¼Œé€‚ç”¨äºç‰¹å®šåœºæ™¯ã€‚"

10. **`--xtc-probability N`**
    - Current: "XTC probability"
    - README: "xtc probability (default: 0.0, 0.0 = disabled)"
    - Target: "XTC probability (default: 0.0, 0.0 = disabled) | XTC æ¦‚ç‡ã€‚æ’é™¤æœ€å¸¸è§ token çš„æ¦‚ç‡ï¼Œç”¨äºå‡å°‘é‡å¤å’Œå¸¸è§çŸ­è¯­ã€‚0.0 è¡¨ç¤ºç¦ç”¨ï¼Œè¾ƒé«˜çš„å€¼å¯ä»¥å¢åŠ æ–‡æœ¬çš„åŸåˆ›æ€§å’Œå¤šæ ·æ€§ã€‚"

11. **`--xtc-threshold N`**
    - Current: "XTC threshold"
    - README: "xtc threshold (default: 0.1, 1.0 = disabled)"
    - Target: "XTC threshold (default: 0.1, 1.0 = disabled) | XTC é˜ˆå€¼ã€‚XTC é‡‡æ ·çš„é˜ˆå€¼å‚æ•°ï¼Œæ§åˆ¶æ’é™¤ token çš„ä¸¥æ ¼ç¨‹åº¦ã€‚1.0 è¡¨ç¤ºç¦ç”¨ã€‚ä¸ xtc-probability é…åˆä½¿ç”¨ï¼Œå…±åŒæ§åˆ¶æ–‡æœ¬ç”Ÿæˆçš„ç‰¹å¾ã€‚"

12. **`--typical N`**
    - Current: "Typical sampling"
    - README: "locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)"
    - Target: "Locally typical sampling, parameter p (default: 1.0, 1.0 = disabled) | å…¸å‹é‡‡æ ·ã€‚å±€éƒ¨å…¸å‹é‡‡æ ·ï¼ŒåŸºäº token çš„å…¸å‹æ€§è¿›è¡Œé€‰æ‹©ã€‚å‚æ•° p æ§åˆ¶å…¸å‹æ€§é˜ˆå€¼ã€‚1.0 è¡¨ç¤ºç¦ç”¨ã€‚é€‚ç”¨äºç”Ÿæˆæ›´åŠ è‡ªç„¶å’Œé¢„æœŸçš„æ–‡æœ¬ã€‚"

13. **`--repeat-last-n N`**
    - Current: "Repeat last N tokens"
    - README: "last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)"
    - Target: "Last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size) | é‡å¤æœ€å N ä¸ª tokenã€‚è€ƒè™‘ç”¨äºæƒ©ç½šé‡å¤çš„æœ€è¿‘ token æ•°é‡ã€‚0 è¡¨ç¤ºç¦ç”¨ï¼Œ-1 è¡¨ç¤ºä½¿ç”¨æ•´ä¸ªä¸Šä¸‹æ–‡å¤§å°ã€‚è¾ƒå¤§çš„å€¼å¯ä»¥æ›´å¥½åœ°é˜²æ­¢é‡å¤ï¼Œä½†å¯èƒ½å½±å“è‡ªç„¶é‡å¤ã€‚"

14. **`--repeat-penalty N`**
    - Current: "Repeat penalty"
    - README: "penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)"
    - Target: "Penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled) | é‡å¤æƒ©ç½šã€‚æƒ©ç½šé‡å¤çš„ token åºåˆ—ï¼Œå‡å°‘æ–‡æœ¬é‡å¤ã€‚1.0 è¡¨ç¤ºç¦ç”¨ï¼Œå¤§äº 1.0 çš„å€¼ä¼šé™ä½é‡å¤ token çš„æ¦‚ç‡ã€‚é€‚ç”¨äºå‡å°‘é‡å¤æ€§å›ç­”å’Œæé«˜æ–‡æœ¬å¤šæ ·æ€§ã€‚"

15. **`--presence-penalty N`**
    - Current: "Presence penalty"
    - README: "repeat alpha presence penalty (default: 0.0, 0.0 = disabled)"
    - Target: "Repeat alpha presence penalty (default: 0.0, 0.0 = disabled) | å­˜åœ¨æƒ©ç½šã€‚alpha å­˜åœ¨æƒ©ç½šï¼ŒåŸºäº token æ˜¯å¦å·²ç»å‡ºç°è¿‡è¿›è¡Œæƒ©ç½šã€‚0.0 è¡¨ç¤ºç¦ç”¨ã€‚æ­£æ•°é¼“åŠ±è®¨è®ºæ–°è¯é¢˜ï¼Œè´Ÿæ•°é¼“åŠ±é‡å¤ã€‚é€‚ç”¨äºä¿æŒå¯¹è¯çš„å¤šæ ·æ€§å’Œæ–°é²œåº¦ã€‚"

16. **`--frequency-penalty N`**
    - Current: "Frequency penalty"
    - README: "repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)"
    - Target: "Repeat alpha frequency penalty (default: 0.0, 0.0 = disabled) | é¢‘ç‡æƒ©ç½šã€‚alpha é¢‘ç‡æƒ©ç½šï¼ŒåŸºäº token å‡ºç°çš„é¢‘ç‡è¿›è¡Œæƒ©ç½šã€‚0.0 è¡¨ç¤ºç¦ç”¨ã€‚æ¯”å­˜åœ¨æƒ©ç½šæ›´ç›´æ¥åœ°æ§åˆ¶é‡å¤ï¼Œé€‚ç”¨äºç²¾ç»†è°ƒæ•´æ–‡æœ¬ç”Ÿæˆè´¨é‡ã€‚"

17. **`--dry-multiplier N`**
    - Current: "DRY multiplier"
    - README: "set DRY sampling multiplier (default: 0.0, 0.0 = disabled)"
    - Target: "Set DRY sampling multiplier (default: 0.0, 0.0 = disabled) | DRY é‡‡æ ·ä¹˜æ•°ã€‚è®¾ç½® DRYï¼ˆDon't Repeat Yourselfï¼‰é‡‡æ ·çš„ä¹˜æ•°ï¼Œæ§åˆ¶æƒ©ç½šå¼ºåº¦ã€‚0.0 è¡¨ç¤ºç¦ç”¨ã€‚é«˜çº§é‡å¤æ£€æµ‹æ–¹æ³•ï¼Œæ¯”ä¼ ç»Ÿé‡å¤æƒ©ç½šæ›´æ™ºèƒ½ã€‚"

18. **`--dry-base N`**
    - Current: "DRY base"
    - README: "set DRY sampling base value (default: 1.75)"
    - Target: "Set DRY sampling base value (default: 1.75) | DRY åŸºç¡€å€¼ã€‚è®¾ç½® DRY é‡‡æ ·çš„åŸºç¡€å€¼ï¼Œå½±å“æƒ©ç½šè®¡ç®—çš„åŸºå‡†ã€‚ä¸ dry-multiplier é…åˆä½¿ç”¨ï¼Œå…±åŒæ§åˆ¶é‡å¤æ£€æµ‹çš„æ•æ„Ÿåº¦ã€‚"

19. **`--dry-allowed-length N`**
    - Current: "DRY allowed length"
    - README: "set allowed length for DRY sampling (default: 2)"
    - Target: "Set allowed length for DRY sampling (default: 2) | DRY å…è®¸é•¿åº¦ã€‚è®¾ç½® DRY é‡‡æ ·å…è®¸çš„é‡å¤åºåˆ—é•¿åº¦ã€‚è¾ƒå°çš„å€¼é™åˆ¶æ›´å¤šï¼Œè¾ƒå¤§çš„å€¼å…è®¸æ›´å¤šçš„è‡ªç„¶é‡å¤ã€‚é€‚ç”¨äºæ§åˆ¶æ–‡æœ¬çš„æµç•…åº¦å’ŒåŸåˆ›æ€§ã€‚"

20. **`--dry-penalty-last-n N`**
    - Current: "DRY penalty last N"
    - README: "set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 = context size)"
    - Target: "Set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 = context size) | DRY æƒ©ç½šæœ€å Nã€‚è®¾ç½® DRY æƒ©ç½šè€ƒè™‘çš„æœ€å N ä¸ª tokenï¼Œ-1 è¡¨ç¤ºä½¿ç”¨ä¸Šä¸‹æ–‡å¤§å°ï¼Œ0 è¡¨ç¤ºç¦ç”¨ã€‚æ§åˆ¶é‡å¤æ£€æµ‹çš„å†å²çª—å£å¤§å°ã€‚"

21. **`--dry-sequence-breaker STRING`**
    - Current: "DRY sequence breaker"
    - README: "add sequence breaker for DRY sampling, clearing out default breakers ('\n', ':', '\"', '*') in the process; use \"none\" to not use any sequence breakers<br/>"
    - Target: "Add sequence breaker for DRY sampling, clearing out default breakers ('\n', ':', '\"', '*') in the process; use \"none\" to not use any sequence breakers | DRY åºåˆ—åˆ†éš”ç¬¦ã€‚æ·»åŠ  DRY é‡‡æ ·çš„åºåˆ—åˆ†éš”ç¬¦ï¼Œæ¸…é™¤é»˜è®¤åˆ†éš”ç¬¦ã€‚ä½¿ç”¨ 'none' ä¸ä½¿ç”¨ä»»ä½•åˆ†éš”ç¬¦ã€‚ç”¨äºè‡ªå®šä¹‰é‡å¤æ£€æµ‹çš„è¾¹ç•Œã€‚"

22. **`--dynatemp-range N`**
    - Current: "Dynamic temperature range"
    - README: "dynamic temperature range (default: 0.0, 0.0 = disabled)"
    - Target: "Dynamic temperature range (default: 0.0, 0.0 = disabled) | åŠ¨æ€æ¸©åº¦èŒƒå›´ã€‚åŠ¨æ€æ¸©åº¦èŒƒå›´ï¼Œæ ¹æ®ç”Ÿæˆä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´æ¸©åº¦ã€‚0.0 è¡¨ç¤ºç¦ç”¨ã€‚å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å†…å®¹è‡ªåŠ¨è°ƒæ•´è¾“å‡ºçš„éšæœºæ€§ï¼Œæä¾›æ›´æ™ºèƒ½çš„ç”Ÿæˆæ§åˆ¶ã€‚"

23. **`--dynatemp-exp N`**
    - Current: "Dynamic temperature exponent"
    - README: "dynamic temperature exponent (default: 1.0)"
    - Target: "Dynamic temperature exponent (default: 1.0) | åŠ¨æ€æ¸©åº¦æŒ‡æ•°ã€‚åŠ¨æ€æ¸©åº¦çš„æŒ‡æ•°å‚æ•°ï¼Œå½±å“æ¸©åº¦è°ƒæ•´çš„æ•æ„Ÿåº¦ã€‚ä¸ dynatemp-range é…åˆä½¿ç”¨ï¼Œæ§åˆ¶åŠ¨æ€æ¸©åº¦çš„è¡Œä¸ºæ¨¡å¼ã€‚"

24. **`--mirostat N`**
    - Current: "Mirostat mode"
    - README: "use Mirostat sampling.<br/>Top K, Nucleus and Locally Typical samplers are ignored if used.<br/>(default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)"
    - Target: "Use Mirostat sampling. Top K, Nucleus and Locally Typical samplers are ignored if used (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) | Mirostat æ¨¡å¼ã€‚ä½¿ç”¨ Mirostat é‡‡æ ·ç®—æ³•ï¼Œè‡ªåŠ¨è°ƒèŠ‚ perplexity åˆ°ç›®æ ‡æ°´å¹³ã€‚0=ç¦ç”¨ï¼Œ1=Mirostatï¼Œ2=Mirostat 2.0ã€‚å¯ç”¨æ—¶å¿½ç•¥ Top Kã€Nucleus å’Œ Locally Typical é‡‡æ ·å™¨ã€‚"

25. **`--mirostat-lr N`**
    - Current: "Mirostat learning rate"
    - README: "Mirostat learning rate, parameter eta (default: 0.1)"
    - Target: "Mirostat learning rate, parameter eta (default: 0.1) | Mirostat å­¦ä¹ ç‡ã€‚Mirostat ç®—æ³•çš„å­¦ä¹ ç‡å‚æ•° etaï¼Œæ§åˆ¶ perplexity è°ƒèŠ‚çš„é€Ÿåº¦ã€‚è¾ƒé«˜çš„å€¼å“åº”æ›´å¿«ä½†å¯èƒ½ä¸ç¨³å®šï¼Œè¾ƒä½çš„å€¼æ›´ç¨³å®šä½†å“åº”è¾ƒæ…¢ã€‚"

26. **`--mirostat-ent N`**
    - Current: "Mirostat entropy"
    - README: "Mirostat target entropy, parameter tau (default: 5.0)"
    - Target: "Mirostat target entropy, parameter tau (default: 5.0) | Mirostat ç†µã€‚Mirostat ç®—æ³•çš„ç›®æ ‡ç†µå‚æ•° tauï¼Œæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„ä¸å¯é¢„æµ‹æ€§ã€‚è¾ƒé«˜çš„å€¼äº§ç”Ÿæ›´å¤šæ ·åŒ–çš„è¾“å‡ºï¼Œè¾ƒä½çš„å€¼äº§ç”Ÿæ›´å¯é¢„æµ‹çš„è¾“å‡ºã€‚"

27. **`-l, --logit-bias TOKEN_ID(+/-)BIAS`**
    - Current: "Logit bias"
    - README: "modifies the likelihood of token appearing in the completion,<br/>i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',<br/>or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'"
    - Target: "Modifies the likelihood of token appearing in the completion, i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello', or `--logit-bias 15043-1` to decrease likelihood of token ' Hello' | Logit åç½®ã€‚ä¿®æ”¹ token åœ¨å®Œæˆä¸­å‡ºç°çš„å¯èƒ½æ€§ï¼Œä¾‹å¦‚ `--logit-bias 15043+1` å¢åŠ  'Hello' token çš„å¯èƒ½æ€§ï¼Œæˆ– `--logit-bias 15043-1` å‡å°‘ 'Hello' token çš„å¯èƒ½æ€§ã€‚ç”¨äºç²¾ç¡®æ§åˆ¶ç”Ÿæˆå†…å®¹çš„ç‰¹å¾ã€‚"

28. **`--grammar GRAMMAR`**
    - Current: "Grammar"
    - README: "BNF-like grammar to constrain generations (see samples in grammars/ dir) (default: '')"
    - Target: "BNF-like grammar to constrain generations (see samples in grammars/ dir) (default: '') | è¯­æ³•ã€‚BNF ç±»è¯­æ³•æ¥çº¦æŸç”Ÿæˆï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆç‰¹å®šæ ¼å¼æˆ–ç»“æ„ï¼ˆå‚è§ grammars/ ç›®å½•ä¸­çš„ç¤ºä¾‹ï¼‰ã€‚é€‚ç”¨äºç”Ÿæˆç»“æ„åŒ–æ•°æ®ã€ä»£ç æˆ–å…¶ä»–éœ€è¦ä¸¥æ ¼æ ¼å¼æ§åˆ¶çš„åœºæ™¯ã€‚"

29. **`--grammar-file FNAME`**
    - Current: "Grammar file"
    - README: "file to read grammar from"
    - Target: "File to read grammar from | è¯­æ³•æ–‡ä»¶ã€‚ä»æ–‡ä»¶è¯»å–è¯­æ³•çº¦æŸã€‚æ”¯æŒä»å¤–éƒ¨æ–‡ä»¶åŠ è½½å¤æ‚çš„è¯­æ³•è§„åˆ™ï¼Œä¾¿äºç®¡ç†å’Œé‡ç”¨è¯­æ³•å®šä¹‰ã€‚"

30. **`-j, --json-schema SCHEMA`**
    - Current: "JSON schema"
    - README: "JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object<br/>For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead"
    - Target: "JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead | JSON æ¶æ„ã€‚JSON æ¶æ„æ¥çº¦æŸç”Ÿæˆï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆ JSON ç»“æ„ï¼ˆä¾‹å¦‚ `{}` è¡¨ç¤ºä»»ä½• JSON å¯¹è±¡ï¼‰ã€‚å¯¹äºåŒ…å«å¤–éƒ¨ $refs çš„æ¶æ„ï¼Œè¯·ä½¿ç”¨ --grammar + example/json_schema_to_grammar.pyã€‚é€‚ç”¨äºç”Ÿæˆ JSON æ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ã€‚"

31. **`-jf, --json-schema-file FILE`**
    - Current: "JSON schema file"
    - README: "File containing a JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object<br/>For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead"
    - Target: "File containing a JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead | JSON æ¶æ„æ–‡ä»¶ã€‚åŒ…å« JSON æ¶æ„çš„æ–‡ä»¶ï¼Œç”¨äºçº¦æŸç”Ÿæˆï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆ JSON ç»“æ„ã€‚æ”¯æŒä»æ–‡ä»¶åŠ è½½å¤æ‚çš„ JSON æ¶æ„å®šä¹‰ã€‚"

---

## Progress Tracking

### Completed Chunks:
- [ ] Chunk 1: Common Parameters (Part 1)
- [ ] Chunk 2: Common Parameters (Part 2) 
- [ ] Chunk 3: Common Parameters (Part 3)
- [ ] Chunk 4: Sampling Parameters
- [ ] Chunk 5: Example-Specific Parameters
- [ ] Chunk 6: Remaining Specialized Parameters

### Total Parameters: ~200+
### Updated Parameters: 0
### Remaining Parameters: ~200+

## Notes:
- Maintain the bilingual format: English | Chinese
- Preserve existing Chinese translations
- Expand English explanations with comprehensive details from README
- Include default values, ranges, environment variables where applicable
- Add use cases and performance implications
- Group related parameters logically for better organization